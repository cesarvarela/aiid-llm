You are an AI assistant that helps classify AI incidents according to a taxonomy.

Your task is to analyze the provided incident text and classify it according to the specified taxonomy.

Always require both the incident text and the taxonomy namespace to perform classification.

Here is the incident text to classify:
It was a cold wind that blew through St. Peter's Square at the Vatican over the weekend, but that didn't deter [Pope Francis](https://www.cbsnews.com/news/pope-francis-hospitalized-vatican-infection/) from taking a stroll outside to greet the faithful, as he often does. When images appeared online showing the 86-year-old pontiff atypically wrapped up against the elements in a stylish white puffer jacket and silver bejewelled crucifix, they soon went viral, racking up millions of views on social media platforms. 

The picture, first published Friday on Reddit along with several others, was in fact a fake. It was an artificial intelligence rendering generated using the AI software Midjourney.

Fake photos generated by artificial intelligence software appear to show Pope Francis walking outside the Vatican in a designer coat, which he never did.

While there are some inconsistencies in the final rendered images --- for example, the pope's left hand where it is holding a water bottle looks distorted and his skin has an overly sharp appearance --- many people online were fooled into thinking they were real pictures.

The revelation that they had been dupped left some Twitter users shocked and confused.

"I thought the pope's puffer jacket was real and didn't give it a second thought," tweeted model and author Chrissy Teigen. "No way am I surviving the future of technology." 

The "pope in the puffer jacket" was just the latest in a series of "deepfake" images created with AI software. Another recent example was pictures of former President Donald Trump that [appeared to show](https://www.bbc.co.uk/news/world-us-canada-65069316) him in police custody. Although the creator made it clear that they were produced as an exercise in the use of AI, the images, combined with [rumors of Trump's imminent arrest](https://www.cbsnews.com/news/ron-desantis-mike-pence-react-to-trumps-claim-hell-be-arrested/), went viral and created and entirely fraudulent but potentially dangerous narrative.

Midjourney, DALL E2, OpenAI and Dream Studio are among the software options available to anyone wishing to produce photo-realistic images using nothing more than text prompts --- no specialist training required.

As this type of software becomes more widespread, AI developers are [working on better ways](https://www.cbsnews.com/news/creating-a-lie-detector-for-deepfakes-artificial-intelligence/) to inform viewers of the authenticity, or otherwise, of images.

CBS News' "Sunday Morning" [reported earlier this year](https://www.cbsnews.com/news/creating-a-lie-detector-for-deepfakes-artificial-intelligence/) that Microsoft's chief scientific officer Eric Horvitz, the co-creator of the spam email filter, was among those trying to crack the conundrum, predicting that if technology isn't developed to enable people to easily detect fakes within a decade or so "most of what people will be seeing, or quite a lot of it, will be synthetic. We won't be able to tell the difference."

In the meantime, Henry Ajder, who presents a BBC radio series entitled, "The Future Will be Synthesised," cautioned in a newspaper interview that it was "already very, very hard to determine whether" some of the images being created were real.

"It gives us a sense of how bad actors, agents spreading disinformation, could weaponize these tools," Ajder told the British newspaper, I.

There's clear evidence of this happening already.

Last March, video emerged appearing to show Ukrainian President Volodymyr Zelenskyy telling his troops to lay down their arms and surrender. It was bad quality and quickly outed as a fake, but it may have been merely an opening salvo in a new information war.

So, while a picture may speak a thousand words, it may be worth asking who's actually doing the talking.

Here is the taxonomy namespace to use for classification:
CSETv1

Here is the taxonomy data:
{
  "namespace": "CSETv1",
  "weight": 70,
  "description": "# What is the CSET Taxonomy?\n\nThe CSET AI Harm Taxonomy for AIID is the second edition of the \nCSET incident taxonomy. It characterizes the harms, entities and \ntechnologies involved in AI incidents and the circumstances of \ntheir occurrence. Every incident is independently classified by \ntwo CSET annotators. Annotations are peer reviewed and finally \nrandomly selected for quality control ahead of publication. \nDespite this rigorous process, mistakes do happen, and readers \nare invited to report any errors they might discover while \nbrowsing. The first version of the CSET taxonomy is available [here](/taxonomy/csetv0/).",
  "field_list": [
    {
      "short_name": "Incident Number",
      "short_description": "The number of the incident in the AI Incident Database.",
      "long_description": "The number of the incident in the AI Incident Database.",
      "permitted_values": null,
      "mongo_type": "int"
    },
    {
      "short_name": "Annotator",
      "short_description": "This is the researcher that is responsible for applying the classifications of the CSET taxonomy.",
      "long_description": "An ID designating the individual who classified this incident according to the CSET taxonomy.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Annotation Status",
      "short_description": "What is the quality assurance status of the CSET classifications for this incident?",
      "long_description": "What is the quality assurance status of the CSET classifications for this incident?",
      "permitted_values": [
        "1. Annotation in progress",
        "2. Initial annotation complete",
        "3. In peer review",
        "4. Peer review complete",
        "5. In quality control",
        "6. Complete and final"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Peer Reviewer",
      "short_description": "This is the researcher that is responsible for ensuring the quality of the classifications applied to this incident.",
      "long_description": "The CSET taxonomy assigns individual researchers to each incident as the primary parties responsible for classifying the incident according to the taxonomy. This is the person responsible for assuring the integrity of annotator's classifications.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Quality Control",
      "short_description": "Has someone flagged a potential issue with this incident's classifications? Annotators should leave this field blank.",
      "long_description": "The peer review process sometimes uncovers issues with the classifications that have been applied by the annotator. This field serves as a flag when there is a need for additional thought and input on the classifications applied",
      "permitted_values": [],
      "mongo_type": "bool"
    },
    {
      "short_name": "Physical Objects",
      "short_description": "Did the incident occur in a domain with physical objects ?",
      "long_description": "“Yes” if the AI system(s) is embedded in hardware that can interact with, affect, and change  the physical objects (cars, robots, medical facilities, etc.). Mark “No” if the system cannot. This includes systems that inform, detect, predict, or recommend.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Entertainment Industry",
      "short_description": "Did the AI incident occur in the entertainment industry?",
      "long_description": "“Yes” if the sector in which the AI was used is associated with entertainment. “No” if it was used in a different, clearly identifiable sector.  “Maybe” if the sector of use could not be determined.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Report, Test, or Study of data",
      "short_description": "Was the incident about a report, test, or study of data instead of the AI itself?",
      "long_description": "“Yes” if the incident is about a report, test, or study of the data and does not discuss an instance of injury, damage, or loss. “Maybe” if it is unclear.  Otherwise mark “No.”",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Deployed",
      "short_description": "Was the reported system (even if AI involvement is unknown) deployed or sold to users?",
      "long_description": "“Yes” if the involved system was deployed or sold to users. “No” if it was not. “Maybe” if there is not enough information or if the use is unclear.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Producer Test in Controlled Conditions",
      "short_description": "Was this a test or demonstration of an AI system done by developers, producers or researchers (versus users) in controlled conditions?",
      "long_description": "“Yes” if it was a test/demonstration performed by developers, producers or journalists in controlled conditions. “No” if it was not a test/demonstration. “No” if the test/demonstration was done by a user. “No” if the test/demonstration was in operational or uncontrolled conditions. “Maybe” otherwise.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Producer Test in Operational Conditions",
      "short_description": "Was this a test or demonstration of an AI system done by developers, producers or researchers (versus users) in operational conditions?",
      "long_description": "“Yes” if it was a test/demonstration performed by developers, producers or journalists in controlled conditions. “No” if it was not a test/demonstration. “No” if the test/demonstration was done by a user. “No” if the test/demonstration was in controlled or non-operational conditions. “Maybe” otherwise.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "User Test in Controlled Conditions",
      "short_description": "Was this a test or demonstration done by users in controlled conditions?",
      "long_description": "“Yes” if it was a test/demonstration performed by users in controlled conditions. “No” if it was not a test/demonstration. “No” if the test/demonstration was done by developers, producers or researchers. “No” if the test/demonstration was in controlled or non-controlled conditions.“Maybe” otherwise.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "User Test in Operational Conditions",
      "short_description": "Was this a test or demonstration done by users in operational conditions?",
      "long_description": "“Yes” if it was a test/demonstration performed by users in operational conditions. “No” if it was not a test/demonstration. “No” if the test/demonstration was done by developers, producers or researchers. “No” if the test/demonstration was in controlled or non-operational conditions.“Maybe” otherwise.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Harm Domain",
      "short_description": "Incident occurred in a domain where we could likely expect harm to occur?",
      "long_description": "Using the answers to the 8 domain questions, assess if the incident occurred in a domain where harm could be expected to occur. If you are unclear, input “maybe.”",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Tangible Harm",
      "short_description": "Did tangible harm (loss, damage or injury ) occur? ",
      "long_description": "An assessment of whether tangible harm, imminent tangible harm, or non-imminent tangible harm occurred. This assessment does not consider the context of the tangible harm, if an AI was involved, or if there is an identifiable, specific, and harmed entity. It is also not assessing if an intangible harm occurred. It is only asking if tangible harm occurred and what its imminency was.",
      "permitted_values": [
        "tangible harm definitively occurred",
        "imminent risk of tangible harm (near miss) did occur",
        "non-imminent risk of tangible harm (an issue) occurred",
        "no tangible harm, near-miss, or issue",
        "unclear"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "AI System",
      "short_description": "Does the incident involve an AI system?",
      "long_description": "An assessment of whether or not an AI system was involved. It is sometimes difficult to judge between an AI and an automated system or expert rules system. In these cases select “maybe”",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Clear link to technology",
      "short_description": "Can the technology be directly and clearly linked to the adverse outcome of the incident",
      "long_description": "An assessment of the technology's involvement in the chain of harm. \"Yes\" indicates that the technology was involved in harm, its behavior can be directly linked to the harm, and the harm may not have occurred if the technology acted differently. \"No\", indicates that the technology's behavior cannot be linked to the harm outcome. \"Maybe\" indicates that the link is unclear.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "There is a potentially identifiable specific entity that experienced the harm",
      "short_description": "A potentially identifiable specific entity that experienced the harm can be characterized or identified.",
      "long_description": "“Yes” if it is theoretically possible to both specify and identify the entity. Having that information is not required. The information just needs to exist and be potentially discoverable. “No” if there are not any potentially identifiable specific entities or if the harmed entities are a class or subgroup that can only be characterized. ",
      "permitted_values": [],
      "mongo_type": "bool"
    },
    {
      "short_name": "AI Harm Level",
      "short_description": "An assessment of the AI tangible harm level, which takes into account the CSET definitions of AI tangible harm levels, along with the inputs for annotation fields about the AI, harm, chain of harm, and entity. ",
      "long_description": "An assessment of the AI tangible harm level, which takes into account the CSET definitions of AI tangible harm levels, along with the inputs for annotation fields about the AI, harm, chain of harm, and entity.",
      "permitted_values": [
        "AI tangible harm event",
        "AI tangible harm near-miss",
        "AI tangible harm issue",
        "none",
        "unclear"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "AI Tangible Harm Level Notes",
      "short_description": "Notes about the AI tangible harm level assessment",
      "long_description": "If for 3.5 you select unclear or leave it blank, please provide a brief description of why.\n\n You can also add notes if you want to provide justification for a level",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Impact on Critical Services",
      "short_description": "Indicates if people’s access to critical public services was impacted.",
      "long_description": "Did this impact people's access to critical or public services (health care, social services, voting, transportation, etc)?",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Rights Violation",
      "short_description": "Indicate if a violation of human rights, civil rights, civil liberties, or democratic norms occurred.",
      "long_description": "Indicate if a violation of human rights, civil rights, civil liberties, or democratic norms occurred.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Involving Minor",
      "short_description": "Was a minor involved in the incident (disproportionally treated or specifically  targeted/affected)",
      "long_description": "Indicate if a minor was disproportionately targeted or affected",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Detrimental Content",
      "short_description": "Was detrimental content (misinformation, hate speech) involved?",
      "long_description": "Detrimental content can include deepfakes, identity misrepresentation, insults, threats of violence, eating disorder or self harm promotion, extremist content, misinformation, sexual abuse material, and scam emails. Detrimental content in itself is often not harmful, however, it can lead to or instigate injury, damage, or loss.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Protected Characteristic",
      "short_description": "Was a group of people treated differently based upon a protected characteristic (e.g. race, ethnicity, creed, immigrant status, color, religion, sex, national origin, age, disability, genetic information)?",
      "long_description": "Protected characteristics include religion, commercial facilities, geography, age, sex, sexual orientation or gender identity, familial status (e.g., having or not having children) or pregnancy, disability, veteran status, genetic information, financial means, race or creed, Ideology, nation of origin, citizenship, and immigrant status.\n\nAt the federal level in the US, age is a protected characteristic for people over the age of 40.  Minors are not considered a protected class.  For this reason the CSET annotation taxonomy  has a separate field to note if a minor was involved.\n\nOnly mark yes if there is clear evidence discrimination occurred. If there are conflicting accounts, mark unsure. Do not mark that discrimination occurred based on expectation alone.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Harm Distribution Basis",
      "short_description": "Indicates how the harms were potentially distributed.",
      "long_description": "Multiple can occur.\n\nGenetic information refers to information about a person’s genetic tests or the genetic tests of their relatives. Genetic information can predict the manifestation of a disease or disorder.",
      "permitted_values": [
        "none",
        "age",
        "disability",
        "familial status (e.g., having or not having children) or pregnancy",
        "financial means",
        "genetic information",
        "geography",
        "ideology",
        "nation of origin, citizenship, immigrant status",
        "race",
        "religion",
        "sex",
        "sexual orientation or gender identity",
        "veteran status",
        "unclear",
        "other"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Notes (special interest intangible harm)",
      "short_description": "Input any notes that may help explain your answers.",
      "long_description": "Input any notes that may help explain your answers.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Special Interest Intangible Harm",
      "short_description": "An assessment of whether a special interest intangible harm occurred. This assessment does not consider the context of the intangible harm, if an AI was involved, or if there is characterizable class or subgroup of harmed entities. It is also not assessing if an intangible harm occurred. It is only asking if a special interest intangible harm occurred.",
      "long_description": "An assessment of whether a special interest intangible harm occurred. This assessment does not consider the context of the intangible harm, if an AI was involved, or if there is characterizable class or subgroup of harmed entities. It is also not assessing if an intangible harm occurred. It is only asking if a special interest intangible harm occurred.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "AI System",
      "short_description": "Does the incident involve an AI system?",
      "long_description": "An assessment of whether or not an AI system was involved. It is sometimes difficult to judge between an AI and an automated system or expert rules system. In these cases select “maybe”",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Clear link to Technology",
      "short_description": "Can the technology be directly and clearly linked to the adverse outcome of the incident?",
      "long_description": "An assessment of the technology's involvement in the chain of harm. \"Yes\" indicates that the technology was involved in harm, its behavior can be directly linked to the harm, and the harm may not have occurred if the technology acted differently. \"No\", indicates that the technology's behavior cannot be linked to the harm outcome. \"Maybe\" indicates that the link is unclear.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Harmed Class of Entities",
      "short_description": "“Yes” if the harmed entity or entities can be characterized. “No” if there are not any characterizable entities.",
      "long_description": "A characterizable class or subgroup are descriptions of different populations of people. Often they are characteristics by which people qualify for special protection by a law, policy, or similar authority.\n\n Sometimes, groups may be characterized by their exposure to the incident via geographical proximity (e.g., ‘visitors to the park’) or participation in an activity (e.g.,‘Twitter users’).",
      "permitted_values": [],
      "mongo_type": "bool"
    },
    {
      "short_name": "Annotator’s AI special interest intangible harm assessment",
      "short_description": "The annotator’s assessment of if an AI special interest intangible harm occurred.",
      "long_description": "AI tangible harm is determined in a different field. The determination of a special interest intangible harm is not dependant upon the AI tangible harm level.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Notes (AI special interest intangible harm)",
      "short_description": "If for 5.5 you select unclear or leave it blank, please provide a brief description of why.\n\nYou can also add notes if you want to provide justification for a level.",
      "long_description": "If for 5.5 you select unclear or leave it blank, please provide a brief description of why.\n\nYou can also add notes if you want to provide justification for a level.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Date of Incident Year",
      "short_description": "The year in which the incident occurred. If there are multiple harms or occurrences of the incident, list the earliest. If a precise date is unavailable, but the available sources provide a basis for estimating the year, estimate. Otherwise, leave blank.\n\nEnter in the format of YYYY",
      "long_description": "The year in which the incident occurred. If there are multiple harms or occurrences of the incident, list the earliest. If a precise date is unavailable, but the available sources provide a basis for estimating the year, estimate. Otherwise, leave blank.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Date of Incident Month",
      "short_description": "The month in which the incident occurred. If there are multiple harms or occurrences of the incident, list the earliest. If a precise date is unavailable, but the available sources provide a basis for estimating the month, estimate. Otherwise, leave blank.\n\nEnter in the format of MM",
      "long_description": "The month in which the incident occurred. If there are multiple harms or occurrences of the incident, list the earliest. If a precise date is unavailable, but the available sources provide a basis for estimating the month, estimate. Otherwise, leave blank.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Date of Incident Day",
      "short_description": "The day on which the incident occurred. If a precise date is unavailable, leave blank.\n\nEnter in the format of DD",
      "long_description": "The day on which the incident occurred. If a precise date is unavailable, leave blank.\n\nEnter in the format of DD",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Estimated Date",
      "short_description": "“Yes” if the data was estimated. “No” otherwise.",
      "long_description": "“Yes” if the data was estimated. “No” otherwise.",
      "permitted_values": [],
      "mongo_type": "bool"
    },
    {
      "short_name": "Multiple AI Interaction",
      "short_description": "“Yes” if two or more independently operating AI systems were involved. “No” otherwise.",
      "long_description": "This happens very rarely but is possible. Examples include two chatbots having a conversation with each other, or two autonomous vehicles in a crash.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Embedded",
      "short_description": "“Yes” if the AI is embedded in a physical system. “No” if it is not. “Maybe” if it is unclear.",
      "long_description": "This question is slightly different from the one in field 2.1.1. That question asks about there being interaction with physical objects–an ability to manipulate or change.  A system can be embedded in a physical object and able to interact with the physical environment, e.g. a vacuum robot.  A system can be embedded in a physical object and not interact with a physical environment, e.g. a camera system that only records images when the AI detects that dogs are present. AI systems that are accessed through API, web-browser, etc by using a mobile device or computer are not considered to be embedded in hardware systems. They are accessed through hardware.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Location City",
      "short_description": "If the incident occurred at a specific known location, note the city.",
      "long_description": "If the incident occurred at a specific known location, note the city. If there are multiple relevant locations, enter multiple city/state/country values.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Location State/Province (two letters)",
      "short_description": "If the incident occurred at a specific known location, note the state/province.",
      "long_description": "If the incident occurred at a specific known location, note the state/province. If there are multiple relevant locations, enter multiple city/state/country values.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Location Country (two letters)",
      "short_description": "If the incident occurred at a specific known location, note the country. Follow ISO 3166 for the 2-letter country codes.",
      "long_description": "Follow ISO 3166 for the 2-letter country codes.\n\nIf there are multiple relevant locations, enter multiple city/state/country values.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Location Region",
      "short_description": "Select the region of the world where the incident occurred. If it occurred in multiple, leave blank.",
      "long_description": "Use this reference to map countries to regions: https://www.dhs.gov/geographic-regions",
      "permitted_values": [
        "Global",
        "Africa",
        "Asia",
        "Caribbean",
        "Central America",
        "Europe",
        "North America",
        "Oceania",
        "South America",
        "unclear"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Infrastructure Sectors",
      "short_description": "Which critical infrastructure sectors were affected, if any?",
      "long_description": "Which critical infrastructure sectors were affected, if any?",
      "permitted_values": [
        "chemical",
        "commercial facilities",
        "communications",
        "critical manufacturing",
        "dams",
        "defense-industrial base",
        "emergency services",
        "energy",
        "financial services",
        "food and agriculture",
        "government facilities",
        "healthcare and public health",
        "information technology",
        "nuclear  ",
        "transportation",
        "water and wastewater",
        "Other",
        "unclear"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Operating Conditions",
      "short_description": "A record of any abnormal or atypical operational conditions that occurred.",
      "long_description": "A record of any abnormal or atypical operational conditions that occurred. This field is most often blank.",
      "permitted_values": [],
      "mongo_type": "array"
    },
    {
      "short_name": "Notes (Environmental and Temporal Characteristics)",
      "short_description": "Input any notes that may help explain your answers.",
      "long_description": "Input any notes that may help explain your answers.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Entities",
      "short_description": "Characterizing Entities and the Harm",
      "long_description": "Characterizing Entities and the Harm",
      "permitted_values": [],
      "mongo_type": "array"
    },
    {
      "short_name": "Lives Lost",
      "short_description": "Indicates the number of deaths reported",
      "long_description": "This field cannot be greater than zero if the harm is anything besides ‘Physical health/safety.’ ",
      "permitted_values": [],
      "mongo_type": "int"
    },
    {
      "short_name": "Injuries",
      "short_description": "Indicate the number of injuries reported.",
      "long_description": "This field cannot be greater than zero if the harm is anything besides 'Physical health/safety'.\n\nAll reported injuries should count, regardless of their severity level. If a person lost their limb and another person scraped their elbow, both cases would be considered injuries. Do not include the number of deaths in this count.",
      "permitted_values": [],
      "mongo_type": "int"
    },
    {
      "short_name": "Estimated Harm Quantities",
      "short_description": "Indicates if the amount was estimated.",
      "long_description": "Indicates if the amount was estimated.",
      "permitted_values": [],
      "mongo_type": "bool"
    },
    {
      "short_name": "Notes ( Tangible Harm Quantities Information)",
      "short_description": "Input any notes that may help explain your answers.",
      "long_description": "Input any notes that may help explain your answers.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "AI System Description",
      "short_description": "A description of the AI system (when possible)",
      "long_description": "Describe the AI system in as much detail as the reports will allow.\n\nA high level description of the AI system is sufficient, but if more technical details about the AI system are available, include them in the description as well.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Data Inputs",
      "short_description": "A list of the types of data inputs for the AI system.",
      "long_description": "This is a freeform field that can have any value. There could be multiple entries for this field.\n\nCommon ones include\n\n- still images\n- video\n- text\n- speech\n- Personally Identifiable Information\n- structured data\n- other\n- unclear\n\nStill images are static images. Video images consist of moving images. Text and speech data are considered an important category of unstructured data. They consist of written and spoken words that are not in a tabular format. Personally identifiable information is data that can uniquely identify an individual and may contain sensitive information. Structured data is often in a tabular, machine readable format and can typically be used by an AI system without much preprocessing.\n\nAvoid using ‘unstructured data’ data in this field. Instead specify the type of unstructured data; text, images, audio files, etc. It is ok to use ‘structured data’ in this field.\n\nRecord what the media report explicitly states. If the report does not explicitly state an input modality but it is likely that a particular kind of input contributed to the harm or near harm, record that input. If you are still unsure, do not record anything.",
      "permitted_values": [],
      "mongo_type": "array"
    },
    {
      "short_name": "Sector of Deployment",
      "short_description": "Indicate the sector in which the AI system is deployed",
      "long_description": "Indicate the sector in which the AI system is deployed\n\nThere could be multiple entries for this field.",
      "permitted_values": [
        "agriculture, forestry and fishing",
        "mining and quarrying",
        "manufacturing",
        "electricity, gas, steam and air conditioning supply",
        "water supply",
        "construction",
        "wholesale and retail trade",
        "transportation and storage",
        "accommodation and food service activities",
        "information and communication",
        "financial and insurance activities",
        "real estate activities",
        "professional, scientific and technical activities",
        "administrative and support service activities",
        "public administration",
        "defense",
        "law enforcement",
        "Education",
        "human health and social work activities",
        "Arts, entertainment and recreation",
        "other service activities",
        "activities of households as employers",
        "activities of extraterritorial organizations and bodies",
        "other",
        "unclear"
      ],
      "mongo_type": "array"
    },
    {
      "short_name": "Public Sector Deployment",
      "short_description": "Indicate whether the AI system is deployed in the public sector",
      "long_description": "Indicate whether the AI system is deployed in the public sector. The public sector is the part of the economy that is controlled and operated by the government.",
      "permitted_values": [
        "yes",
        "no",
        "maybe"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Autonomy Level",
      "short_description": "Autonomy1: The system operates independently without simultaneous human oversight, interaction, or intervention.\n\nAutonomy2: The system operates independently but with human oversight, where a human can observe and override the system’s decisions in real time.\n\nAutonomy3: The system does not independently make decisions but instead provides information to a human who actively chooses to proceed with the AI’s information.",
      "long_description": "Autonomy1: The system operates independently without simultaneous human oversight, interaction, or intervention.\n\nAutonomy2: The system operates independently but with human oversight, where a human can observe and override the system’s decisions in real time.\n\nAutonomy3: The system does not independently make decisions but instead provides information to a human who actively chooses to proceed with the AI’s information.",
      "permitted_values": [
        "Autonomy1",
        "Autonomy2",
        "Autonomy3",
        "unclear"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Notes (Information about AI System)",
      "short_description": "Input any notes that may help explain your answers.",
      "long_description": "Input any notes that may help explain your answers.",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "Intentional Harm",
      "short_description": "Was the AI intentionally developed or deployed to perform the harm?\n\nIf yes, did the AI’s behavior result in unintended or intended harm? ",
      "long_description": "Indicates if the system was designed to do harm.  If it was designed to perform harm, the field will indicate if the AI system did or did not create unintended harm–i.e. was the reported harm the harm that AI was expected to perform or a different unexpected harm? ",
      "permitted_values": [
        "Yes. Intentionally designed to perform harm and did create intended harm",
        "Yes. Intentionally designed to perform harm but created an unintended harm (a different harm may have occurred)",
        "No. Not intentionally designed to perform harm",
        "unclear"
      ],
      "mongo_type": "string"
    },
    {
      "short_name": "Physical System Type",
      "short_description": "Describe the type of physical system that the AI was integrated into.",
      "long_description": "Describe the type of physical system that the AI was integrated into. ",
      "permitted_values": [],
      "mongo_type": "string"
    },
    {
      "short_name": "AI Task",
      "short_description": "Describe the AI’s application.",
      "long_description": "Describe the AI’s application.\n\nIt is likely that the annotator will not have enough information to complete this field. If this occurs, enter unclear.\n\nThis is a freeform field. Some possible entries are\n\n- unclear\n- human language technologies\n- computer vision\n- robotics\n- automation and/or optimization\n- other\n\nThe application area of an AI is the high level task that the AI is intended to perform. It does not describe the technical methods by which the AI performs the task. Considering what an AI’s technical methods enable it to do is another way of arriving at what an AI’s application is. \n\nIt is possible for multiple application areas to be involved. When possible pick the principle or domain area, but it is ok to select multiple areas.",
      "permitted_values": [],
      "mongo_type": "array"
    },
    {
      "short_name": "AI tools and methods",
      "short_description": "Describe the tools and methods that enable the AI’s application.",
      "long_description": "Describe the tools and methods that enable the AI’s application.\n\nIt is likely that the annotator will not have enough information to complete this field. If this occurs, enter unclear\n\nThis is a freeform field. Some possible entries are\n\n- unclear\n- reinforcement learning\n- neural networks\n- decision trees\n- bias mitigation\n- optimization\n- classifier\n- NLP/text analytics\n- continuous learning\n- unsupervised learning\n- supervised learning\n- clustering\n- prediction\n- rules\n- random forest\n\nAI tools and methods are the technical building blocks that enable the AI’s application.",
      "permitted_values": [],
      "mongo_type": "array"
    },
    {
      "short_name": "Notes (AI Functionality and Techniques)",
      "short_description": "Input any notes that may help explain your answers.",
      "long_description": "Input any notes that may help explain your answers.",
      "permitted_values": [],
      "mongo_type": "string"
    }
  ]
}

Here are similar incidents and their classifications:
Id: 110
title: Arkansas's Opaque Algorithm to Allocate Health Care Excessively Cut Down Hours for Beneficiaries
description: Beneficiaries of the Arkansas Department of Human Services (DHS)'s Medicaid waiver program were allocated excessively fewer hours of caretaker visit via an algorithm deployed to boost efficiency, which reportedly contained errors and whose outputs varied wildly despite small input changes.

first report text: Artificial intelligence (AI) and algorithmic decision-making systems — algorithms that analyze massive amounts of data and make predictions about the future — are increasingly affecting Americans’ daily lives. People are compelled to include [buzzwords](https://www.zipjob.com/blog/get-your-resume-past-applicant-tracking-systems/) in their resumes to get past AI-driven hiring software. Algorithms are deciding who will get housing or financial loan opportunities. And biased [testing software](https://www.insidehighered.com/news/2021/02/01/u-illinois-says-goodbye-proctorio) is forcing students of color and students with disabilities to grapple with increased anxiety that they may be locked out of their exams or flagged for cheating. But there’s another frontier of AI and algorithms that should worry us greatly: the use of these systems in medical care and treatment.

The use of AI and algorithmic decision-making systems in medicine are increasing even though current regulation may be insufficient to detect harmful racial biases in these tools. Details about the tools’ development are largely unknown to clinicians and the public — a lack of transparency that threatens to automate and worsen racism in the health care system. Last week, the [FDA issued guidance](https://www.fda.gov/media/109618/download) significantly broadening the scope of the tools it plans to regulate. This broadening guidance emphasizes that more must be done to combat bias and promote equity amid the growing number and increasing use of AI and algorithmic tools.

In 2019, a [bombshell study](https://www.science.org/doi/10.1126/science.aax2342) found that a clinical algorithm many hospitals were using to decide which patients need care was showing racial bias — Black patients had to be deemed much sicker than white patients to be recommended for the same care. This happened because the algorithm had been trained on past data on health care spending, which reflects a history in which Black patients had less to spend on their health care compared to white patients, due to longstanding wealth and income disparities. While this algorithm’s bias was eventually detected and corrected, the incident raises the question of how many more clinical and medical tools may be similarly discriminatory.

Another algorithm, created to determine how many hours of aid Arkansas residents with disabilities would receive each week, was criticized after [making extreme cuts](https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy) to in-home care. Some residents attributed extreme disruptions to their lives and even hospitalization to the sudden cuts. A resulting lawsuit found that several errors in the algorithm — errors in how it characterized the medical needs of people with certain disabilities — were directly to blame for inappropriate cuts made. Despite this outcry, the group that developed the flawed algorithm still creates tools used in health care settings in nearly half of U.S. states as well as internationally.

[One recent study](https://news.emory.edu/stories/2022/05/hs_ai_systems_detect_patient_race_27-05-2022/story.html) found that an AI tool trained on medical images, like x-rays and CT scans, had unexpectedly learned to discern patients’ self-reported race. It learned to do this even when it was trained only with the goal of helping clinicians diagnose patient images. This technology’s ability to tell patients’ race — even when their doctor cannot — could be abused in the future, or unintentionally direct worse care to communities of color without detection or intervention.

Some algorithms used in the clinical space are severely under-regulated in the U.S. The U.S Department of Health and Human Services (HHS) and its subagency the Food and Drug Administration (FDA) are tasked with regulating medical devices — with devices ranging from a tongue depressor to a pacemaker and now, medical AI systems. While some of these medical devices (including AI) and tools that aid physicians in treatment and diagnosis are regulated, other algorithmic decision-making tools used in clinical, administrative, and public health settings — such as those that predict risk of mortality, likelihood of readmission, and in-home care needs — are not required to be reviewed or regulated by the FDA or any regulatory body.

This lack of oversight can lead to biased algorithms being used widely by hospitals and state public health systems, contributing to increased discrimination against Black and Brown patients, people with disabilities, and other marginalized communities. In some cases, this failure to regulate can lead to wasted money and lives lost. One such AI tool, developed to detect sepsis early, is used by more than 170 hospitals and health systems. But a [recent study](https://www.pewtrusts.org/en/research-and-analysis/articles/2021/12/16/artificial-intelligence-can-improve-health-care-but-not-without-human-oversight) revealed the tool failed to predict this life-threatening illness in 67 percent of patients who developed it, and generated false sepsis alerts on thousands of patients who did not. Acknowledging this failure was the result of under-regulation, the FDA’s new guidelines point to these tools as examples of products it will now regulate as medical devices.

The FDA’s approach to regulating drugs, which involves publicly shared data that is scrutinized by review panels for adverse effects and events contrasts to its approach to regulating medical AI and algorithmic tools. Regulating medical AI presents a novel issue and will require considerations that differ from those applicable to the hardware devices the FDA is used to regulating. These devices include [pulse oximeters](https://www.npr.org/sections/health-shots/2022/07/11/1110370384/when-it-comes-to-darker-skin-pulse-oximeters-fall-short/), [thermal thermometers](https://news.emory.edu/stories/2022/09/hs_bhavani_jama_racial_differences_thermometers_detecting_fevers_06-09-2022/story.html), and [scalp electrodes](https://nam04.safelinks.protection.outlook.com/GetUrlReputation)—each of which have been found to reflect racial or ethnic bias in how well they function in subgroups. News of these biases only underscores how vital it is to properly regulate these tools and ensure they don’t perpetuate bias against vulnerable racial and ethnic groups.

While the FDA suggests that device manufacturers test their devices for racial and ethnic biases before marketing to the general public, this step is not required. Perhaps more important than assessments after a device is developed is transparency during its development. A [STAT+ News study](https://www.statnews.com/2021/02/11/breast-cancer-disparities-artificial-intelligence-fda/) found many AI tools approved or cleared by the FDA do not include information about the diversity of the data on which the AI was trained, and that the number of these tools being cleared is [increasing rapidly](https://www.statnews.com/2021/02/03/fda-clearances-artificial-intelligence-data/). Another [study](https://www.nature.com/articles/s41591-021-01595-0) found AI tools “consistently and selectively under-diagnosed under-served patient populations,” finding the under-diagnosis rate was higher for marginalized communities who disproportionately don’t have access to medical care. This is unacceptable when these tools may make decisions that have life or death consequences.

Equitable treatment by the health care system is a civil rights issue. The COVID-19 pandemic has laid bare the many ways in which existing societal inequities produce health care inequities — a complex reality that humans can attempt to comprehend, but that is difficult to accurately reflect in an algorithm. The promise of AI in medicine was that it could help remove bias from a deeply biased institution and improve health care outcomes; instead, it threatens to automate this bias.

Policy changes and collaboration among key stakeholders, including state and federal regulators, medical, public health, and clinical advocacy groups and organizations, are needed to address these gaps and inefficiencies. To start, as detailed in a [new ACLU white paper](https://www.aclu.org/legal-document/aclu-white-paper-ai-health-care-may-worsen-medical-racism):

*   Public reporting of demographic information should be required.
*   The FDA should require an impact assessment of any differences in device performance by racial or ethnic subgroup as part of the clearance or approval process.
*   Device labels should reflect the results of this impact assessment.
*   The FTC should collaborate with HHS and other federal bodies to establish best practices that device manufacturers not under FDA regulation should follow to lessen the risk of racial or ethnic bias in their tools.

Rather than learn of racial and ethnic bias embedded in clinical and medical algorithms and devices from bombshell publications revealing what amounts to medical and clinical malpractice, the HHS and FDA and other stakeholders must work to ensure that medical racism becomes a relic of the past rather than a certainty of the future.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"none\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"human health and social work activities\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"tangible harm definitively occurred\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy1\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "\"\""
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"4. Peer review complete\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "110"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"The report states that a patient in Idaho had to be hospitalized following the cut in at-home care hours assigned by the algorithm. However, the incident talks about an 'algorithm' and does not mention AI, Machine Learning, or models. It is likely an expert based on statistics serviced algorithm and therefore does not meet the CSET definition for AI.\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"Access to health care is a UN-defined human right. Therefore, this is a human rights violation--even if it does not meet the definition of CSET AI harm because CSET's definition for AI was not met.\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"The first incident just talks about an 'algorithm' and does not mention AI, Machine Learning, or models.  It is likely an expert based on statistics serviced algorithm and therefore, does not meet the CSET definition for AI. Therefore, it does not meet the CSET definition of AI intangible harm.\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2011"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"AR; ID\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[\"healthcare and public health\"]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"Idaho developed and implemented their algorithm to determine at-home care needs in 2011. Arkansas deployed their algorithm in 2016\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"InterRAI\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"non-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Idaho welfare recipients\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Bradley Ledgerwood\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Tammy Dobbs\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Arkansas Medicaid waiver program beneficiaries\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Arkansas Department of Human Services\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"government entity\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Arkansas healthcare workers\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"user\\\"]\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Unnamed  beneficiary of the Medicaid waiver program\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"physical health/safety\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"The report notes that an unnamed beneficiary of the waiver program was hospitalized because the algorithm had cut the hours of care they received. \\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Legal Aid \\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"non-profit organization\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Attorney Kevin De Liban represented patients in court against the use of the algorithm. \\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"watchdog\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "true"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"It is unclear how many injuries or hospitalizations were caused because of the hour cuts mandated by the algorithm\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"The algorithm allocates medicate services based on recipient data.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"symptoms\",\"health data\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 10
title: Kronos Scheduling Algorithm Allegedly Caused Financial Issues for Starbucks Employees
description: Kronos’s scheduling algorithm and its use by Starbucks managers allegedly negatively impacted financial and scheduling stability for Starbucks employees, which disadvantaged wage workers.

first report text: In April, the New York attorney general's office launched an [investigation](http://www.reuters.com/article/2015/04/13/us-retail-workers-nyag-idUSKBN0N40G420150413) into the scheduling practices of 13 national retail chains, distributing a letter to the Gap, Target, J.C. Penney, and 10 other companies. The letter asked, among other things, whether these companies' store managers use software manufactured by a company called Kronos to algorithmically generate schedules.

A few months later, Kronos was also featured prominently in an [article](http://www.nytimes.com/interactive/2014/08/13/us/starbucks-workers-scheduling-hours.html) published by the _New York Times_ about the ill effects of erratic scheduling on Starbucks employees, especially one particular family. In a [follow-up piece](http://www.nytimes.com/times-insider/2014/08/22/times-article-changes-a-policy-fast/), the author, Jodi Kantor, points directly to Kronos' scheduling software as the root of the problem. "I saw that her life was coming apart and that the Starbucks software had contributed to the crisis," Kantor wrote of one of the story's subjects.

The piece's argument centered around the financial and scheduling unpredictability engendered by platforms like Kronos. When you don't know if your shift might be canceled, if or when you'll be called in, or what your hours will look like next week or the week after, it becomes [very difficult](http://www.buzzfeed.com/sapna/victorias-secret-keeps-workers-on-call-and-unpaid#.wspVMPngB) to make even the most

basic plans for your future. This can have devastating long-term financial and emotional impacts on workers. According to a [recent study](http://www.epi.org/publication/irregular-work-scheduling-and-its-consequences/) by the Economic Policy Institute, a left-leaning think tank in Washington, D.C., 17 percent of the American workforce is negatively affected by unstable schedules.

For their part, Kronos representatives argue that the algorithm is far from the root of the problem. "The populist view is that scheduling is evil, in that it's causing erratic schedules for employees, and so forth," Charlie DeWitt, vice president of business development for Kronos, told BuzzFeed News. "The fact of the matter is it's an algorithm. It does whatever you want it to do."

And you don't necessarily need to work for Kronos to believe that in a competitive retail climate, the problem is more complicated than technology alone. [Lonnie Golden](http://www.abington.psu.edu/academics/faculty/dr-lonnie-golden), a Penn State economist who has extensively studied the impact of erratic scheduling, acknowledges that Kronos' product itself is less to blame than the managers who make staffing decisions based on the data it provides. "It's not necessarily the technology that's responsible for minimum to no advance notice," he said. "It's the way in which it's applied."

But, he added, "where there's a technology problem, there's usually a technology solution." And while Kronos maintains that managers, and not the software, are responsible for early dismissals and last-minute shift cancellations, the company is nonetheless pursuing some technological solutions.

Kronos wants to help managers better understand how scheduling adjustments affect workers and, ultimately, the bottom line. Though the company maintains that its software doesn't produce the kind of erratic schedules that hurt wage workers, DeWitt said there was nonetheless an interest in figuring out why that perception existed — and, if possible, fixing it.

To that end, earlier this month at a [retail conference in Philadelphia](http://www.kronos.com/microsites/RetailExecSummitSpring15/), the company announced that it's working on a new plug-in that will give managers better insight into workers' schedule stability, equity of hours worked among employees, and the consistency of schedules from week to week. In addition, Kronos is improving a feature meant to help give employees more control over their schedules: Though the software already incorporates employee availability and preferences into its scheduling calculations, improvements to a shift-swapping feature on its employee-facing web and mobile apps will theoretically allow employees to work around conflicts among themselves.

Golden said increased employee input and control would be a good thing. But some retailers, DeWitt pointed out, are uncomfortable making workers use an app outside of work hours; indeed, the practice could be seen as a shift of management responsibilities onto lower-paid individuals.

Part of the idea behind the new Kronos plug-in is to help companies tie fairer scheduling practices to reduction in absenteeism and turnover, which can be enormously costly. In other words, if Kronos can help executives see the connection between treating workers fairly and a store's ability to increase revenue, DeWitt said, managers will have an impetus to create more predictable, stable schedules.

And just because companies are looking at this kind of data doesn't mean they have to use it. "Companies like Kronos and Workplace Systems are starting to integrate some of these principles into their software," said [Carrie Gleason](http://populardemocracy.org/carrie-gleason), director of the Fair Workweek Initiative at the Center for Popular Democracy, "but it's all optional, so companies can decide not to do it." While [12 states](http://populardemocracy.org/campaign/restoring-fair-workweek) are currently considering legislation that would create new labor standards around the workweek, Gleason said the technology alone lacks a mechanism for enforcement.

Given market pressures and standard management practices, it's unlikely that any change to Kronos' technology would give workers more power — especially because, given the [competitive retail climate](http://www.buzzfeed.com/sapna/retail-winter-of-death#.krGlE3qDm) at the moment, the bottom line tends to be the priority. "It's not just bad managers. They have extreme pressure to increase productivity on an ever-shrinking labor budget," Gleason said.

With these changes, Kronos has taken logical steps toward both repairing its reputation and making sure its software creates sustainable work environments. But while the company cannot control exactly how the algorithm that forecasts schedules and optimizes workforces is deployed inside different workplaces, the Kronos engineers who designed the product are nonetheless the partial architects of work environments that have been proven to be untenable for low-wage workers. The Kronos scheduling algorithm isn't designed to serve those people; it's designed to be sold to their bosses, and as such, will ultimately be shaped to serve the needs of management — until regulations exist that compel them to change how it's used.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"none\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"accommodation and food service activities\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"tangible harm definitively occurred\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"AI tangible harm event\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy2\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "\"\""
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "10"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"no\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2014"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "true"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Starbucks\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Kronos\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Kronos scheduling algorithm\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Starbucks employees\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - It is reasonable to expect that unpredictable schedules have led to financial loss for other Starbucks employees through lost wages or unexpected expenses like childcare to attend work on short notice.\\\"\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI tangible harm issue\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"financial loss\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Kylei Weisse\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI tangible harm event\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"financial loss\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Weisse incurred unexpected financial costs in order to make it to a shift that was assigned to him on very short notice. \\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Starbucks employees\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"other intangible harm\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - Unpredictable schedules cause stress through financial and scheduling instability\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Jannette Navarro\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"other intangible harm\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - Her unpredictable schedules caused serious stress and instability in her life.\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"The Kronos scheduling algorithm is designed to optimize the productivity of stores like Starbucks by scheduling workers inconsistently throughout and across weeks based on predicted store traffic.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"schedules\",\"worker profiles\",\"store traffic\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"scheduling\",\"productivity optimization\",\"predict store traffic\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 79
title: Kidney Testing Method Allegedly Underestimated Risk of Black Patients
description: Decades-long use of the estimated glomerular filtration rate (eGFR) method to test kidney function which considers race has been criticized by physicians and medical students for its racist history and inaccuracy against Black patients.

first report text: **BACKGROUND:** Advancing health equity entails reducing disparities in care. African-American patients with chronic kidney disease (CKD) have poorer outcomes, including dialysis access placement and transplantation. Estimated glomerular filtration rate (eGFR) equations, which assign higher eGFR values to African-American patients, may be a mechanism for inequitable outcomes. Electronic health record–based registries enable population-based examination of care across racial groups.

**OBJECTIVE:** To examine the impact of the race multiplier for African-Americans in the CKD-EPI eGFR equation on CKD classification and care delivery.

**DESIGN:** Cross-sectional study

**SETTING:** Two large academic medical centers and affiliated community primary care and specialty practices.

**PARTICIPANTS:** A total of 56,845 patients in the Partners HealthCare System CKD registry in June 2019, among whom 2225 (3.9%) were African-American.

**MEASUREMENT:** Exposures included race, age, sex, comorbidities, and eGFR. Outcomes were transplant referral and dialysis access placement.

**RESULTS:** Of 2225 African-American patients, 743 (33.4%) would hypothetically be reclassified to a more severe CKD stage if the race multiplier were removed from the CKD-EPI equation. Similarly, 167 of 687 (24.3%) would be reclassified from stage 3B to stage 4. Finally, 64 of 2069 patients (3.1%) would be reassigned from eGFR > 20 ml/min/1.73 m2 to eGFR ≤ 20 ml/min/1.73 m2, meeting the criterion for accumulating kidney transplant priority. Zero of 64 African-American patients with an eGFR ≤ 20 ml/min/1.73 m2 after the race multiplier was removed were referred, evaluated, or waitlisted for kidney transplant, compared to 19.2% of African-American patients with eGFR ≤ 20 ml/min/1.73 m2 with the default CKD-EPI equation.

**LIMITATIONS:** Single healthcare system in the Northeastern United States and relatively small African-American patient cohort may limit generalizability.

**CONCLUSIONS:** Our study reveals a meaningful impact of race-adjusted eGFR on the care provided to the African-American CKD patient population.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"race\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"human health and social work activities\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"tangible harm definitively occurred\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy3\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "\"\""
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"3. In peer review\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "79"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"There is no AI. The harm comes from a formula that uses race as a factor.\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"4.1 - Black patients overlooked by the calculation because of built-in points had their access to critical public healthcare reduced.\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"5.3 - Though there was no AI, the technology involved can be linked to the adverse outcomes in the incident.\\n5.5 - Because there is no AI, this incident does not qualify for CSET's definition of AI special interest intangible harm.\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2009"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "true"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[\"healthcare and public health\"]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"According to the incident report, \\\"Researchers who created the formula in 2009 added the “race correction” to smooth out statistical differences between the small number of Black patients and others in their data.\\\" \""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Black patients with chronic kidney disease\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"physical health/safety\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Black patients with chronic kidney disease\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"CKD-EPI eGFR calculation\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product not containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"physicians\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"National Kidney Foundation\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"non-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"researcher\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"American Society of Nephrology\\\"\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"non-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"researcher\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Paloma Orozco Scott\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"watchdog\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"medical institutions\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"infrastructure\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Black patients with chronic kidney disease\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "true"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"In June 2019, it was estimated how many Black Americans, at that point in time, were negatively affected by the algorithm using race. The estimate just looked at a small portion of the patients in the US, those at Mass General Brigham health system. The research estimated that 64 additional Black Americans would have qualified to be referred, evaluated, or waitlisted for a kidney transplant if the race factor was removed from the equation. Additional 743, would have been classified at a more severe stage if the race factor was removed. Since this equation has been used for about 30 years throughout health institutions in the US, 10s of thousands of Black Americans were likely affected.\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"There is no AI. The harm comes from a formula that was developed in the 1990s and uses race as a factor.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"age\",\"sex\",\"race\",\"creatinine levels\",\"medical data\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"not AI\""
  }

---

Id: 96
title: Houston Schools Must Face Teacher Evaluation Lawsuit
description: On May 4, 2017, a U.S. federal judge advanced teachers’ claims that the Houston Independent School District’s algorithmic teacher evaluations violated their due process rights to their jobs by not allowing them to review the grounds of their termination.

first report text: HOUSTON (CN) – A proprietary system that measures teacher performance based on student test scores may violate teachers’ civil rights because they can’t verify the results are accurate, a federal judge ruled, advancing a lawsuit against Texas’ largest school district.

Houston Independent School District has more than 215,000 students and 283 schools. It is the seventh largest school district in the United States.

Opposition to U.S. school districts’ widespread practice of judging teachers’ skills based on their students’ standardized test scores has intensified in recent years, with critics saying such exams force teachers to curtail their lesson plans, tailoring them to the test questions, while encouraging memorization rather than critical thinking in students.

Houston ISD jumped on the big-data bandwagon in 2011 when it signed a license agreement with the SAS Institute, a North Carolina-based multinational, to use the company’s Educational Value-Added Assessment System, or EVAAS.

The system tracks teachers’ impact with a proprietary algorithm that compares their students’ test results to the statewide average for students in that grade or course.

In the first lawsuit in the Fifth Circuit to allege such systems violate teachers’ Fourteenth Amendment procedural due process rights to their jobs, the Houston Federation of Teachers Local 2415 and six Houston ISD teachers sued the district in April 2014. The Fifth Circuit has jurisdiction over federal courts in Louisiana, Mississippi and Texas.

Shortly after implementing the system in 2012, Houston ISD announced a goal of firing 85 percent of teachers it rated as ineffective, the case record states.

The union, which has more than 6,100 members, argues in court filings that the system violates Fifth Circuit precedent and Texas law, which gives teachers on the chopping block a right to hear evidence about why the district has chosen to fire them, with enough detail for them to show the decision was made in error.

That detail is lacking in the SAS Institute’s software because the company deems its algorithms as trade secrets and refuses to share them Houston ISD or its teachers, so teachers have no way of knowing if an error in the program has decreased their scores.

U.S. Magistrate Judge Stephen Smith sided with the teachers, refusing to dismiss their procedural due process claims in a May 4 opinion.

“The EVAAS score might be erroneously calculated for any number of reasons, ranging from data-entry mistakes to glitches in the computer code itself. Algorithms are human creations, and subject to error like any other human endeavor,” Smith wrote.

The union successfully compared its case to claims made by air-traffic controllers in Banks v. Federal Aviation Administration before the Fifth Circuit.

The controllers were fired after lab tests of their urine samples showed trace amounts of cocaine. They argued they were denied due process because the lab had destroyed their samples and they could not independently test them to see if it made a mistake. The New Orleans-based appeals court agreed in a 1982 ruling that gave the controllers their jobs back.

“Plaintiffs assert that Banks is controlling here, and that due process similarly requires an opportunity by teachers to test on their own behalf the accuracy of their HISD-sponsored value-added scores. The court agrees,” Smith wrote.

The school district meanwhile claimed that 42 states and the District of Columbia lump student performance into teacher evaluations and that such systems have been vetted and widely endorsed by academics.

Smith partially sided with the district, dismissing the union’s substantive due process claims.

He found the union could not prove the evaluation system is unconstitutionally vague, or that there’s no rational link between teacher scores and HISD’s goal of “having an effective teacher in every HISD classroom.”

Plaintiff Andy Dewey taught history at HISD’s Carnegie Vanguard High School for nearly 40 years before retiring after the lawsuit was filed in 2014. He said he’s hopeful the ruling will motivate Houston ISD to start settlement talks.

“My thought is that if HISD is wise they will sit down and meet with us and try to come to a settlement. I can’t say that they’ll do that so I don’t know if it’s going to go to trial, but from the judge’s wording it seems like we have a very strong case,”  Dewey said on Monday by phone.

Dewey is now business manager and executive vice president of the Houston Federation of Teachers Local 2415. He is one of nine current or former HISD teachers who are co-plaintiffs in their first amended lawsuit, which they filed in February 2015.

The district ended its contract with SAS in 2016 and did not replace the teacher-evaluation system with other software.

“They haven’t taken it off the table though as far as using a similar system or coming back and using EVAAS again, so if they do want to sit down and settle that’s one thing we’ll be talking about,” Dewey said.

Judge Smith echoed that concern in his ruling and explained why the contract termination did not moot the lawsuit.

“The voluntary cessation of allegedly illegal conduct does not render a case moot. … HISD concedes that it is investigating all of its options for value-added modeling going forward, including SAS’s EVAAS product,” Smith wrote.

Houston ISD did not immediately respond Monday when asked why it terminated the SAS contract. Its spokeswoman said Courthouse News would have to submit a public-information request to find out how many teachers it fired during the contract period.

The district’s attorney, Chelsea Glover with Gibson Dunn and Crutcher in Dallas, would not comment on the ruling.

Houston voters on Saturday agreed to cut Texas a $77.5 million check this year to subsidize less property-wealthy school districts, rather than the more expensive option of detaching some of the city’s most valuable properties from the tax rolls and sending those taxes to poorer districts.

Houston ISD officials say the state’s “Robin Hood” school-funding system unfairly penalizes districts in big cities by taking funds it needs to educate its students, more than 75 percent of whom come from “economically disadvantaged” families.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"none\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"Education\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"non-imminent risk of tangible harm (an issue) occurred\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy3\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"response model\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"4. Peer review complete\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "96"
  }
  {
    "short_name": "Annotator",
    "value_json": "null"
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"3.5 - the value-added measurement/modeling is not AI - it is a statistical model\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"A proprietary system that measures teacher performance based on student test scores may violate teachers’ civil rights because they can’t verify the results are accurate, a federal judge ruled, advancing a lawsuit against Texas’ largest school district.\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2012"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"Houston\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"TX\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[\"Other\"]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\" The lawsuit was first filed in 2014 after the system was implemented in 2012 and used until 2016. U.S. Magistrate Judge Stephen Smith sided with teachers in a decision on May 4, 2017.\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"SAS Institute\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Educational Value-Added Assessment System (EVAAS)\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product not containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Houston Independent School District teachers\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"financial loss\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Because teachers couldn't verify the accuracy of the algorithm's output, there is a reasonable probability that a teacher was wrongfully let go based on the algorithmic assessment. \\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Houston Independent School District teachers\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Houston Independent School District\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"government entity\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"EVAAS is a teacher performance assessment system that tracks teachers’ impact with a proprietary algorithm that compares their students’ test results to the statewide average for students in that grade or course.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"student data\",\"student test results\",\"standardized test results\",\"statewide average test scores\",\"worker performance data\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"The value-added system provides data about teachers which is used by the school district to make firing decisions.\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"none\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"prediction\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"not AI\""
  }

---

Id: 47
title: LinkedIn Search Prefers Male Names
description: An investigation by The Seattle Times in 2016 found a gender bias in LinkedIn's search engine.

first report text: LinkedIn.

Last week, a Seattle Times investigation revealed that LinkedIn’s search function seems to have a pretty pronounced gender bias. It turns out, when you search for a woman’s name on LinkedIn, the site has a pesky habit of asking whether you’re actually looking for a similarly spelled man’s name instead.

For example, on a search for “Stephanie Williams,” LinkedIn helpfully asks if you meant to look up “Stephan Williams” instead — even though, according to the Seattle Times, there are approximately 2,500 profile results for the name “Stephanie Williams.” A similar pattern holds for at least a dozen common American women’s names. Type in “Andrea” — are you sure you didn’t mean Andrew? And “Michaela” — what about Michael? “Danielle” — Daniel, right?

Curiously, searches for the 100 most-common U.S. male names did not result in any suggestions for their female counterparts.

LinkedIn maintains that its algorithm doesn’t have a gender bias, explaining that the suggested male names are simply based on the site’s most-common searches. But today the BBC reports that LinkedIn has updated its search function to avoid proposing any alternative names, which sounds like a good plan!

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"sex\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"information and communication\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"no tangible harm, near-miss, or issue\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "false"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy1\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "\"\""
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "47"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"002\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"LinkedIn's search suggestion algorithm prompted users searching for female names to choose similar-sounding male names instead.\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2016"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"08\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "31"
  }
  {
    "short_name": "Estimated Date",
    "value_json": "true"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"maybe\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"The date refers to the date of publication of the investigation revealing the bias.\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Women\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Seattle Times\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"watchdog\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"A Seattle Times investigation uncovered the bias in LinkedIn's search suggestions.\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"LinkedIn\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\",\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"Search engine recommender system suggesting alternative search terms (names) for professional networking.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"text\",\"names\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"prediction\",\"recommendation\",\"search suggestion\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 43
title: Racist AI behaviour is not a new problem
description: From 1982 to 1986, St George's Hospital Medical School used a program to automate a portion of their admissions process that resulted in discrimination against women and members of ethnic minorities.

first report text: Companies and governments need to pay attention to the unconscious and institutional biases that seep into their algorithms, argues cybersecurity expert Megan Garcia. Distorted data can skew results in web searches, home loan decisions, or photo recognition software. But the combination of increased attention to the inputs, greater clarity about the properties of the code itself, and the use of crowd-level monitoring could contribute to a more equitable online world. Without careful consideration, Garcia writes, our technology will be just as racist, sexist, and xenophobic as we are.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"nation of origin, citizenship, immigrant status\",\"sex\",\"race\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"Education\",\"human health and social work activities\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"no tangible harm, near-miss, or issue\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"maybe\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"maybe\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy3\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"natural language processing\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "43"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"002\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"The Commission for Racial Equality found St. George's Hospital Medical School guilty of discrimination against women and members of ethnic minorities.\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "1979"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"London\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"GB\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"Europe\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Medical school applicants\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Medical school applicants\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Admissions algorithm\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product maybe containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"St. George's Hospital Medical School\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"other\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.3 - medical school\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Commission for Racial Equality\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"government entity\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.3 - \\\\\\\"non-departmental public body\\\\\\\"\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"government oversight\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Dr. Franglen\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"Applicant screening algorithm used in the first stage of admissions process, scoring and ranking applications for interview selection.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"Text\",\"applicant data\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"9.5 The computer used applicants' information to generate a score which was used to decide which applicants should be interviewed. Although the system operated fully independently, it only automated the first stage of the admissions process. Human decisions were involved in the next stages of the process and in making the final decisions on acceptance.\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"Rank Applicants\",\"application screening\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"algorithmic, but unclear if AI. \""
  }

---

Id: 11
title: Northpointe Risk Models
description: An algorithm developed by Northpointe and used in the penal system is two times more likely to incorrectly label a black person as a high-risk re-offender and is two times more likely to incorrectly label a white person as low-risk for reoffense according to a ProPublica review.

first report text: Across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant’s likelihood of becoming a recidivist – a term used to describe criminals who re-offend. There are dozens of these risk assessment algorithms in use. Many states have built their own assessments, and several academics have written tools. There are also two leading nationwide tools offered by commercial vendors.

We set out to assess one of the commercial tools made by Northpointe, Inc. to discover the underlying accuracy of their recidivism algorithm and to test whether the algorithm was biased against certain groups.

Our analysis of Northpointe’s tool, called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions), found that black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk.

We looked at more than 10,000 criminal defendants in Broward County, Florida, and compared their predicted recidivism rates with the rate that actually occurred over a two-year period. When most defendants are booked in jail, they respond to a COMPAS questionnaire. Their answers are fed into the COMPAS software to generate several scores including predictions of “Risk of Recidivism” and “Risk of Violent Recidivism.”

We compared the recidivism risk categories predicted by the COMPAS tool to the actual recidivism rates of defendants in the two years after they were scored, and found that the score correctly predicted an offender’s recidivism 61 percent of the time, but was only correct in its predictions of violent recidivism 20 percent of the time.

In forecasting who would re-offend, the algorithm correctly predicted recidivism for black and white defendants at roughly the same rate (59 percent for white defendants, and 63 percent for black defendants) but made mistakes in very different ways. It misclassifies the white and black defendants differently when examined over a two-year follow-up period.

Our analysis found that:

Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent).

White defendants were often predicted to be less risky than they were. Our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders (48 percent vs. 28 percent).

The analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.

Black defendants were also twice as likely as white defendants to be misclassified as being a higher risk of violent recidivism. And white violent recidivists were 63 percent more likely to have been misclassified as a low risk of violent recidivism, compared with black violent recidivists.

The violent recidivism analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 77 percent more likely to be assigned higher risk scores than white defendants.

Previous Work

In 2013, researchers Sarah Desmarais and Jay Singh examined 19 different recidivism risk methodologies being used in the United States and found that “in most cases, validity had only been examined in one or two studies conducted in the United States, and frequently, those investigations were completed by the same people who developed the instrument.”

Their analysis of the research published before March2013 found that the tools “were moderate at best in terms of predictive validity,” Desmarais said in an interview. And she could not find any substantial set of studies conducted in the United States that examined whether risk scores were racially biased. “The data do not exist,” she said.

The largest examination of racial bias in U.S. risk assessment algorithms since then is a 2016 paper by Jennifer Skeem at University of California, Berkeley and Christopher T. Lowenkamp from the Administrative Office of the U.S. Courts. They examined data about 34,000 federal offenders to test the predictive validity of the Post Conviction Risk Assessment tool that was developed by the federal courts to help probation and parole officers determine the level of supervision required for an inmate upon release.

The authors found that the average risk score for black offenders was higher than for white offenders, but that concluded the differences were not attributable to bias.

A 2013 study analyzed the predictive validity among various races for another score called the Level of Service Inventory, one of the most popular commercial risk scores from Multi-Health Systems. That study found that “ethnic minorities have higher LS scores than nonminorities.” The study authors, who are Canadian, noted that racial disparities were more consistently found in the U.S. than in Canada. “One possibility may be that systematic bias within the justice system may distort the measurement of ‘true’ recidivism,” they wrote.

A smaller 2006 study of 532 male residents of a work-release program also found “a tendency toward classification errors for African Americans” in the Level of Service Inventory-Revised. The study, by Kevin Whiteacre of the Salvation Army Correctional Services Program, found that 42.7 percent of African Americans were incorrectly classified as high risk, compared with 27.7 percent of Caucasians and 25 percent of Hispanics. That study urged correctional facilities to investigate the their use of the scores independently using a simple contingency table approach that we follow later in this study.

As risk scores move further into the mainstream of the criminal justice system, policy makers have called for further studies of whether the scores are biased.

When he was U.S. Attorney General, Eric Holder asked the U.S. Sentencing Commission to study potential bias in the tests used at sentencing. “Although these measures were crafted with the best of intentions, I am concerned that they inadvertently undermine our efforts to ensure individualized and equal justice,” he said, adding, “they may exacerbate unwarranted and unjust disparities that are already far too common in our criminal justice system and in our society.” The sentencing commission says it is not currently conducting an analysis of bias in risk assessments.

So ProPublica did its own analysis.

How We Acquired the Data

We chose to examine the COMPAS algorithm because it is one of the most popular scores used nationwide and is increasingly being used in pretrial and sentencing, the so-called “front-end” of the criminal justice system. We chose Broward County because it is a large jurisdiction using the COMPAS tool in pretrial release decisions and Florida has strong open-records laws.

Through a public records request, ProPublica obtained two years worth of COMPAS scores from the Broward County Sheriff’s Office in Florida. We received data for all 18,610 people who were scored in 2013 and 2014.

Because Broward County primarily uses the score to determine whether to release or detain a defendant before his or her trial, we discarded scores that were assessed at parole, probation or other stages in the criminal justice system. That left us with 11,757 people who were assessed at the pretrial stage.

Each pretrial defendant received at least three COMPAS scores: “Risk of Recidivism,” “Risk of Violence” and “Risk of Failure to Appear.”

COMPAS scores for each defendant ranged from 1 to 10, with ten being the highest risk. Scores 1 to 4 were labeled by COMPAS as “Low”; 5 to 7 were labeled “Medium”; and 8 to 10 were labeled “High.”

Starting with the database of COMPAS scores, we built a profile of each person’s criminal history, both before and after they were scored. We collected public criminal records from the Broward County Clerk’s Office website through April 1, 2016. On average, defendants in our dataset were not incarcerated for 622.87 days (sd: 329.19).

We matched the criminal records to the COMPAS records using a person’s first and last names and date of birth. This is the same technique used in the Broward County COMPAS validation study conducted by researchers at Florida State University in 2010. We downloaded around 80,000 criminal records from the Broward County Clerk’s Office website.

To determine race, we used the race classifications used by the Broward County Sheriff’s Office, which identifies defendants as black, white, Hispanic, Asian and Native American. In 343 cases, the race was marked as Other.

We also compiled each person’s record of incarceration. We received jail records from the Broward County Sheriff’s Office from January 2013 to April 2016, and we downloaded public incarceration records from the Florida Department of Corrections website.

We found that sometimes people’s names or dates of birth were incorrectly entered in some records – which led to incorrect matches between an individual’s COMPAS score and his or her criminal records. We attempted to determine how many records were affected. In a random sample of 400 cases, we found an error rate of 3.75 percent (CI: +/- 1.8 percent).

How We Defined Recidivism

Defining recidivism was key to our analysis.

In a 2009 study examining the predictive power of its COMPAS score, Northpointe defined recidivism as “a finger-printable arrest involving a charge and a filing for any uniform crime reporting (UCR) code.” We interpreted that to mean a criminal offense that resulted in a jail booking and took place after the crime for which the person was COMPAS scored.

It was not always clear, however, which criminal case was associated with an individual’s COMPAS score. To match COMPAS scores with accompanying cases, we considered cases with arrest dates or charge dates within 30 days of a COMPAS assessment being conducted. In some instances, we could not find any corresponding charges to COMPAS scores. We removed those cases from our analysis.

Next, we sought to determine if a person had been charged with a new crime subsequent to crime for which they were COMPAS screened. We did not count traffic tickets and some municipal ordinance violations as recidivism. We did not count as recidivists people who were arrested for failing to appear at their court hearings, or people who were later charged with a crime that occurred prior to their COMPAS screening.

For violent recidivism, we used the FBI’s definition of violent crime, a category that includes murder, manslaughter, forcible rape, robbery and aggravated assault.

For most of our analysis, we defined recidivism as a new arrest within two years. We based this decision on Northpointe’s practitioners guide, which says that its recidivism score is meant to predict “a new misdemeanor or felony offense within two years of the COMPAS administration date.”

In addition, a recent study of 25,000 federal prisoners’ recidivism rates by the U.S. Sentencing Commission, which shows that most recidivists commit a new crime within the first two years after release (if they are going to commit a crime at all).

Analysis

We analyzed the COMPAS scores for “Risk of Recidivism” and “Risk of Violent Recidivism.” We did not analyze the COMPAS score for “Risk of Failure to Appear.”

We began by looking at the risk of recidivism score. Our initial analysis looked at the simple distribution of the COMPAS decile scores among whites and blacks. We plotted the distribution of these scores for 6,172 defendants who had not been arrested for a new offense or who had recidivated within two years.


These histograms show that scores for white defendants were skewed toward lower-risk categories, while black defendants were evenly distributed across scores. In our two-year sample, there were 3,175 black defendants and 2,103 white defendants, with 1,175 female defendants and 4,997 male defendants. There were 2,809 defendants who recidivated within two years in this sample.

The histograms for COMPAS’s violent risk score also show a disparity in score distribution between white and black defendants. The sample we used to test COMPAS’s violent recidivism score was slightly smaller than for the general recidivism score: 4,020 defendants, 1,918 black defendants and 1,459 white defendants. There were 652 violent recidivists.


While there is a clear difference between the distributions of COMPAS scores for white and black defendants, merely looking at the distributions does not account for other demographic and behavioral factors.

To test racial disparities in the score controlling for other factors, we created a logistic regression model that considered race, age, criminal history, future recidivism, charge degree, gender and age.

Risk of General Recidivism Logistic Model

Dependent variable:

Score (Low vs Medium and High)

Female	0.221*** (0.080)

Age: Greater than 45	-1.356*** (0.099)

Age: Less than 25	1.308*** (0.076)

Black	0.477*** (0.069)

Asian	-0.254 (0.478)

Hispanic	-0.428*** (0.128)

Native American	1.394* (0.766)

Other	-0.826*** (0.162)

Number of Priors	0.269*** (0.011)

Misdemeanor	-0.311*** (0.067)

Two year Recidivism	0.686*** (0.064)

Constant	-1.526*** (0.079)

Observations	6,172

Akaike Inf. Crit.	6,192.402

Note: *p<0.1; **p<0.05; ***p<0.01

We used those factors to model the odds of getting a higher COMPAS score. According to Northpointe’s practitioners guide, COMPAS “scores in the medium and high range garner more interest from supervision agencies than low scores, as a low score would suggest there is little risk of general recidivism,” so we considered scores any higher than “low” to indicate a risk of recidivism.

Our logistic model found that the most predictive factor of a higher risk score was age. Defendants younger than 25 years old were 2.5 times as likely to get a higher score than middle aged offenders, even when controlling for prior crimes, future criminality, race and gender.

Race was also quite predictive of a higher score. While Black defendants had higher recidivism rates overall, when adjusted for this difference and other factors, they were 45 percent more likely to get a higher score than whites.

Surprisingly, given their lower levels of criminality overall, female defendants were 19.4 percent more likely to get a higher score than men, controlling for the same factors.

Risk of Violent Recidivism Logistic Model

Dependent variable:

Score (Low vs Medium and High)

Female	-0.729*** (0.127)

Age: Greater than 45	-1.742*** (0.184)

Age: Less than 25	3.146*** (0.115)

Black	0.659*** (0.108)

Asian	-0.985 (0.705)

Hispanic	-0.064 (0.191)

Native American	0.448 (1.035)

Other	-0.205 (0.225)

Number of Priors	0.138*** (0.012)

Misdemeanor	-0.164* (0.098)

Two Year Recidivism	0.934*** (0.115)

Constant	-2.243*** (0.113)

Observations	4,020

Akaike Inf. Crit.	3,022.779

Note: *p<0.1; **p<0.05; ***p<0.01

The COMPAS software also has a score for risk of violent recidivism. We analyzed 4,020 people who were scored for violent recidivism over a period of two years (not including time spent incarcerated). We ran a similar regression model for these scores.

Age was an even stronger predictor of a higher score for violent recidivism. Our regression showed that young defendants were 6.4 times more likely to get a higher score than middle age defendants, when correcting for criminal history, gender, race and future violent recidivism.

Race was also predictive of a higher score for violent recidivism. Black defendants were 77.3 percent more likely than white defendants to receive a higher score, correcting for criminal history and future violent recidivism.

To test COMPAS’s overall predictive accuracy, we fit a Cox proportional hazards model to the data – the same technique that Northpointe used in its own validation study. A Cox model allows us to compare rates of recidivism while controlling for time. Because we aren’t controlling for other factors such as a defendant’s criminality we can include more people in this Cox model. For this analysis our sample size was 10,314 defendants (3,569 white defendants and 5,147 black defendants).

Risk of General Recidivism Cox Model

High Risk	1.250*** (0.041)

Medium Risk	0.796*** (0.041)

Observations	13,344

R2	0.068

Max. Possible R2	0.990

Wald Test	954.820*** (df = 2)

LR Test	942.824*** (df = 2)

Score (Logrank) Test	1,054.767*** (df = 2)

Note: *p<0.1; **p<0.05; ***p<0.01

We considered people in our data set to be “at risk” from the day they were given the COMPAS score until the day they committed a new offense or April 1, 2016, whichever came first. We removed people from the risk set while they were incarcerated. The independent variable in the Cox model was the COMPAS categorical risk score.

The Cox model showed that people with high scores were 3.5 times as likely to recidivate as people in the low (scores 1 to 4) category. Northpointe’s study, found that people with high scores (scores 8 to 10) were 5.6 times as likely to recidivate. Both results indicate that the score has predictive value.

A Kaplan Meier survival plot also shows a clear difference in recidivism rates between each COMPAS score level.


Overall, the Cox regression had a concordance score of 63.6 percent. That means for any randomly selected pair of defendants in the sample, the COMPAS system can accurately rank their recidivism risk 63.6 percent of the time (e.g. if one person of the pair recidivates, that pair will count as a successful match if that person also had a higher score). In its study, Northpointe reported a slightly higher concordance: 68 percent.

Running the Cox model on the underlying risk scores - ranked 1 to 10 - rather than the low, medium and high intervals yielded a slightly higher concordance of 66.4 percent.

Both results are lower than what Northpointe describes as a threshold for reliability. “A rule of thumb according to several recent articles is that AUCs of .70 or above typically indicate satisfactory predictive accuracy, and measures between .60 and .70 suggest low to moderate predictive accuracy,” the company says in its study.

The COMPAS violent recidivism score had a concordance of 65.1 percent.

The COMPAS system unevenly predicts recidivism between genders. According to Kaplan-Meier estimates, women rated high risk recidivated at a 47.5 percent rate during two years after they were scored. But men rated high risk recidivated at a much higher rate – 61.2 percent – over the same time period. This means that a high-risk woman has a much lower risk of recidivating than a high-risk man, a fact that may be overlooked by law enforcement officials interpreting the score.


Northpointe does offer a custom test for women, but it is not in use in Broward County.

The predictive accuracy of the COMPAS recidivism score was consistent between races in our study – 62.5 percent for white defendants vs. 62.3 percent for black defendants. The authors of the Northpointe study found a small difference in the concordance scores by race: 69 percent for white defendants and 67 percent for black defendants.

Across every risk category, black defendants recidivated at higher rates.


Risk of General Recidivism Cox Model (with Interaction Term)

Black	0.279*** (0.061)

Asian	-0.777 (0.502)

Hispanic	-0.064 (0.097)

Native American	-1.255 (1.001)

Other	0.014 (0.110)

High Score	1.284*** (0.084)

Medium Score	0.843*** (0.071)

Black:High	-0.190* (.100, p: 0.0574)

Asian:High	1.316* (0.768)

Hispanic:High	-0.119 (0.198)

Native American:High	1.956* (.083)

Other:High	0.415 (0.259)

Black:Medium	-0.173* (.091, p: 0.0578)

Asian:Medium	0.986 (0.711)

Hispanic:Medium	0.065 (0.164)

Native American:Medium	1.390 (1.120)

Other:Medium	-0.334 (0.232)

Observations	13,344

R2	0.072

Max. Possible R2	0.990

Log Likelihood	-30,280.410

Wald Test	988.830*** (df = 17)

LR Test	993.709*** (df = 17)

Score (Logrank) Test	1,104.894*** (df = 17)

Note: *p<0.1; **p<0.05; ***p<0.01

We also added a race-by-score interaction term to the Cox model. This term allowed us to consider whether the difference in recidivism between a high score and low score was different for black defendants and white defendants.

The coefficient on high scores for black defendants is almost statistically significant (0.0574). High-risk white defendants are 3.61 times as likely to recidivate as low-risk white defendants, while high-risk black defendants are only 2.99 times as likely to recidivate as low-risk black defendants. The hazard ratios for medium-risk defendants vs. low risk defendants also are different across races: 2.32 for white defendants and 1.95 for black defendants. Because of the gap in hazard ratios, we can conclude that the score is performing differently among racial subgroups.

We ran a similar analysis on COMPAS’s violent recidivism score, however we did not find a similar result. Here, we found that the interaction term on race and score was not significant, meaning that there is no significant difference the hazards of high and low risk black defendants and high and low risk white defendants.

Overall, there are far fewer violent recidivists than general recidivists and there isn’t a clear difference in the hazard rates across score levels for black and white recidivists. These Kaplan Meier plots show very low rates of violent recidivism.


Finally, we investigated whether certain types of errors – false positives and false negatives – were unevenly distributed among races. We used contingency tables to determine those relative rates following the analysis outlined in the 2006 paper from the Salvation Army.

We removed people from our data set for whom we had less than two years of recidivism information. The remaining population was 7,214 – slightly larger than the sample in the logistic models above, because we don’t need a defendant’s case information for this analysis. As in the logistic regression analysis, we marked scores other than “low” as higher risk. The following tables show how the COMPAS recidivism score performed:

All Defendants

Low	High

Survived	2681	1282

Recidivated	1216	2035

FP rate: 32.35

FN rate: 37.40

PPV: 0.61

NPV: 0.69

LR+: 1.94

LR-: 0.55

Black Defendants

Low	High

Survived	990	805

Recidivated	532	1369

FP rate: 44.85

FN rate: 27.99

PPV: 0.63

NPV: 0.65

LR+: 1.61

LR-: 0.51

White Defendants

Low	High

Survived	1139	349

Recidivated	461	505

FP rate: 23.45

FN rate: 47.72

PPV: 0.59

NPV: 0.71

LR+: 2.23

LR-: 0.62

These contingency tables reveal that the algorithm is more likely to misclassify a black defendant as higher risk than a white defendant. Black defendants who do not recidivate were nearly twice as likely to be classified by COMPAS as higher risk compared to their white counterparts (45 percent vs. 23 percent). However, black defendants who scored higher did recidivate slightly more often than white defendants (63 percent vs. 59 percent).

The test tended to make the opposite mistake with whites, meaning that it was more likely to wrongly predict that white people would not commit additional crimes if released compared to black defendants. COMPAS under-classified white reoffenders as low risk 70.5 percent more often than black reoffenders (48 percent vs. 28 percent). The likelihood ratio for white defendants was slightly higher 2.23 than for black defendants 1.61.

We also tested whether restricting our definition of high risk to include only COMPAS’s high score, rather than including both medium and high scores, changed the results of our analysis. In that scenario, black defendants were three times as likely as white defendants to be falsely rated at high risk (16 percent vs. 5 percent).

We found similar results for the COMPAS violent recidivism score. As before, we calculated contingency tables based on how the score performed:

All Defendants

Low	High

Survived	4121	1597

Recidivated	347	389

FP rate: 27.93

FN rate: 47.15

PPV: 0.20

NPV: 0.92

LR+: 1.89

LR-: 0.65

Black defendants

Low	High

Survived	1692	1043

Recidivated	170	273

FP rate: 38.14

FN rate: 38.37

PPV: 0.21

NPV: 0.91

LR+: 1.62

LR-: 0.62

White defendants

Low	High

Survived	1679	380

Recidivated	129	77

FP rate: 18.46

FN rate: 62.62

PPV: 0.17

NPV: 0.93

LR+: 2.03

LR-: 0.77

Black defendants were twice as likely as white defendants to be misclassified as a higher risk of violent recidivism, and white recidivists were misclassified as low risk 63.2 percent more often than black defendants. Black defendants who were classified as a higher risk of violent recidivism did recidivate at a slightly higher rate than white defendants (21 percent vs. 17 percent), and the likelihood ratio for white defendants was higher, 2.03, than for black defendants, 1.62.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"race\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"law enforcement\",\"public administration\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"tangible harm definitively occurred\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"AI tangible harm event\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy3\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "11"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"ProPublica found evidence of differential performance rates of the COMPAS recidivism risk prediction system across racial groups. The system was both used in sentencing (affecting judge’s decisions concerning the length of sentences) and pretrial bond hearings (affecting decisions concerning pretrial detainment and bond amounts). There is a reasonable probability that the risk scores inaccurately aggravated choices made by judges. Moreover, there is at least one instance in which a judge admitted to assigning a longer prison sentence due to the elevated risk score, which was reduced on appeal.\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"ProPublica found evidence of differential performance rates of the COMPAS recidivism risk prediction system across racial groups. The system was both used in sentencing (affecting judge’s decisions concerning the length of sentences) and pretrial bond hearings (affecting decisions concerning pretrial detainment and bond amounts). There is a reasonable probability that the risk scores wrongfully aggravated choices made by judges. \""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2013"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"ProPublica\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"non-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"watchdog\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Northpointe\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"COMPAS\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Defendants assessed by COMPAS\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - ProPublica established that the algorithm was biased and disproportionately determined black people were more at risk for recidivism.\\\"\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Defendants assessed by COMPAS\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - Wrongful arrest, and unequal treatment before the law is a violation of human rights, civil liberties, civil rights, or democratic norms.\\\"\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"violation of human rights, civil liberties, civil rights, or democratic norms\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Defendants assessed by COMPAS\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI tangible harm issue\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"financial loss\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - Convicts misjudged by COMPAS may suffer a non-imminent risk of financial loss through higher bail from the AI decision that they would not have incurred otherwise\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Defendants assessed by COMPAS\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"other tangible harm\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Wrongful arrest or detainment harms physical freedom and autonomy\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Paul Zilly\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI tangible harm event\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"other tangible harm\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Zilly's risk score impacted the judge’s sentencing, which was corrected down on appeal; the judge himself stated that had it not been for the score, his sentencing would have been much shorter.\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Brisha Borden\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Non-AI tangible harm event.  It is likely that her elevated recidivism risk scores caused her bond to be raised from 0$ to 1000$, but we cannot establish a clear link.\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"financial loss\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"COMPAS deployers\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"government entity\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Deployers of COMPAS include New York State Division of Criminal Justice Services, Wisconsin Department of Corrections, Broward County, Florida\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Sade Jones\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"financial loss\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Non-AI tangible harm event.  It is likely that her elevated recidivism risk score caused her bond to be raised from 0$ to 1000$, but we cannot establish a clear link.\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"The COMPAS system calculates a person's risk of recidivism based on their criminal records and responses to a 137-questions long survey about the situation and context of the crime and the person involved.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"criminal record\",\"questionnaire responses\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"predict recidivism\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 13
title: High-Toxicity Assessed on Text Involving Women and Minority Groups
description: Google's Perspective API, which assigns a toxicity score to online text, seems to award higher toxicity scores to content involving non-white, male, Christian, heterosexual phrases.

first report text: According to a 2019 Pew Center survey, the majority of respondents believe the tone and nature of political debate in the U.S. have become more negative and less respectful. This observation has motivated scientists to study the civility or lack thereof in political discourse, particularly on broadcast television. Given their ability to parse language at scale, one might assume that AI and machine learning systems might be able to aid in these efforts. But researchers at the University of Pennsylvania find that at least one tool, Jigsaw’s Perspective API, clearly isn’t up to the task.

Incivility is more subtle and nuanced than toxicity, for example, which includes identity slurs, profanity, and threats of violence. While incivility detection is a well-established task in AI, it’s not well-standardized, with the degree and type of incivility varying across datasets.

The researchers studied Perspective — an AI-powered API for content moderation developed by Jigsaw, the organization working under Google parent company Alphabet to tackle cyberbullying and disinformation — in part because of its widespread use. Media organizations including the New York Times, Vox Media, OpenWeb, and Disqus have adopted it, and it’s now processing 500 million requests daily.

To benchmark Perspective’s ability to spot incivility, the researchers built a corpus containing 51 transcripts from PBS NewsHour, MSNBC’s The Rachel Maddow Show, and Hannity from Fox News. Annotators read through each transcript and identified segments that appeared to be especially uncivil or civil, rating them on a ten-point scale for measures like “polite/rude,” “friendly/hostile,” “cooperative/quarrelsome,” and “calm/agitated.” Scores and selections across annotators were composited to net a civility score for each snippet between 1 and 10, where 1 is the most civil and 10 is the least civil possible.

After running the annotated transcript snippets through the Perspective API, the researchers found that the API wasn’t sensitive enough to detect differences in levels of incivility for ratings lower than six. Perspective scores increased for higher levels of incivility, but annotator and Perspective incivility scores only agreed 51% of the time.

“Overall, for broadcast news, Perspective cannot reproduce the incivility perception of people,” the researchers write. “In addition to the inability to detect sarcasm and snark, there seems to be a problem with over-prediction of the incivility in PBS and FOX [programming].”

In a subsequent test, the researchers sampled thousands of words from each transcript, gathering a total of 2,671, which they fed to Perspective to predict incivility. The results show a problematic trend: Perspective tends to label certain identities — including “gay,” “African-American,” “Muslim” and “Islam,” “Jew,” “women,” and “feminism” and “feminist” — as toxic. Moreover, the API erroneously flags words relating to violence and death (e.g., “die,” “kill,” “shooting,” “prostitution,” “pornography,” “sexual”) even in the absence of incivility, as well as words that in one context could be toxic but in another could refer to a name (e.g., “Dick”).

Other auditors have claimed that Perspective doesn’t moderate hate and toxic speech equally across groups of people. A study published by researchers at the University of Oxford, the Alan Turing Institute, Utrecht University, and the University of Sheffield found that the Perspective API particularly struggles with denouncements of hate that quote others’ hate speech or make direct references to it. An earlier University of Washington study published in 2019 found that Perspective was more likely to label “Black-aligned English” offensive versus “white-aligned English.”

For its part, Jigsaw recently told VentureBeat that it has made and continues to make progress toward mitigating the biases in its models.

The researchers say that their work highlights the shortcomings of AI when applied to the task of civility detection. While they believe that prejudices against groups like Muslims and African Americans can be lessened through “data-driven” techniques, they expect that correctly classifying edge cases like sarcasm will require the development of new systems.

“The work we presented was motivated by the desire to apply off-the-shelf methods for toxicity prediction to analyse civility in American news. These methods were developed to detect rude, disrespectful, or unreasonable comment that is likely to make you leave the discussion in an online forum,” the coauthors wrote. “We find that Perspective’s inability to differentiate levels of incivility is partly due to the spurious correlations it has formed between certain non-offensive words and incivility. Many of these words are identity-related. Our work will facilitate future research efforts on debiasing of automated predictions.”

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"sex\",\"sexual orientation or gender identity\",\"religion\",\"race\",\"nation of origin, citizenship, immigrant status\",\"disability\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"information and communication\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"no tangible harm, near-miss, or issue\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "false"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy1\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"natural language processing\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "13"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"Google's sister company, Jigsaw, created a tool called \\\"Perspective\\\" that uses machine learning to classify the toxicity level of a given phrase or sentence. Individuals and researchers accessing the product through an API demonstrated the model's inability to reliably categorize harmful sentences as toxic and innocuous phrases as non-toxic. In particular, the model's output exhibited ableist, racist, sexist, and homophobic tendencies. \""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2017"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "true"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Perspective API\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Toxicity classification model.\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Women\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Perspective AI judges comments that make positive, not normative, statements such as \\\\\\\"I am a woman\\\\\\\" as more toxic than gender-neutral phrases and male-identifying phrases. \\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Jigsaw\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Partner organizations\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"collaborators\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"The New York Times, Wikipedia, The Economist, The Guardian collaborated with Jigsaw in the development of the Perspective model\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Various online media outlets\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"The New York Times, Vox Media, OpenWeb, and Disqus use Perspective to rate and moderate comments on their websites. \\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Google\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Minority groups\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Groups include the LGBTQ+ community, disabled people, Black people, religious minorities in the US, Mexicans, and others. \\\\nPerspective AI judges comments that make positive, not normative, statements such as \\\\\\\"I am gay\\\\\\\", \\\\\\\"I am disabled\\\\\\\", \\\\\\\"I am Black\\\\\\\" as more toxic than comments about other sexual orientations, ability or races.\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"Google's Perspective project, a machine learning-based system to identify toxic comments in online discussion forums\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"text\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"toxicity detection\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"The system uses a combination of natural language processing techniques and machine learning algorithms to analyze text and identify language that is toxic, where toxic is defined as \\\"a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.\\\"\""
  }

---

Id: 14
title: Biased Sentiment Analysis
description: Google Cloud's Natural Language API provided racist, homophobic, amd antisemitic sentiment analyses.

first report text: Google's code of conduct explicitly prohibits discrimination based on sexual orientation, race, religion, and a host of other protected categories. However, it seems that no one bothered to pass that information along to the company's artificial intelligence.

The Mountain View-based company developed what it's calling a Cloud Natural Language API, which is just a fancy term for an API that grants customers access to a machine-learning powered language analyzer which allegedly "reveals the structure and meaning of text." There's just one big, glaring problem: The system exhibits all kinds of bias.

First reported by Motherboard, the so-called "Sentiment Analysis" offered by Google is pitched to companies as a way to better understand what people really think about them. But in order to do so, the system must first assign positive and negative values to certain words and phrases. Can you see where this is going?

The system ranks the sentiment of text on a -1.0 to 1.0 scale, with -1.0 being "very negative" and 1.0 being "very positive." On a test page, inputting a phrase and clicking "analyze" kicks you back a rating.

"You can use it to extract information about people, places, events and much more, mentioned in text documents, news articles or blog posts," reads Google's page. "You can use it to understand sentiment about your product on social media or parse intent from customer conversations happening in a call center or a messaging app."

Both "I'm a homosexual" and "I'm queer" returned negative ratings (-0.5 and -0.1, respectively), while "I'm straight" returned a positive score (0.1).

Image: Google

And it doesn't stop there, "I'm a jew" and "I'm black" returned scores of -0.1.

Image: google

Interestingly, shortly after Motherboard published their story, some results changed. A search for "I'm black" now returns a neutral 0.0 score, for example, while "I'm a jew" actually returns a score of -0.2 (i.e., even worse than before).

"White power," meanwhile, is given a neutral score of 0.0.

Image: google

So what's going on here? Essentially, it looks like Google's system picked up on existing biases in its training data and incorporated them into its readings. This is not a new problem, with an August study in the journal Science highlighting this very issue.

We reached out to Google for comment, and the company both acknowledged the problem and promised to address the issue going forward.

"We dedicate a lot of efforts to making sure the NLP API avoids bias, but we don’t always get it right," a spokesperson wrote to Mashable. "This is an example of one of those times, and we are sorry. We take this seriously and are working on improving our models. We will correct this specific case, and, more broadly, building more inclusive algorithms is crucial to bringing the benefits of machine learning to everyone.”

So where does this leave us? If machine learning systems are only as good as the data they're trained on, and that data is biased, Silicon Valley needs to get much better about vetting what information we feed to the algorithms. Otherwise, we've simply managed to automate discrimination — which I'm pretty sure goes against the whole "don't be evil" thing.

This story has been updated to include a statement from Google.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"religion\",\"sexual orientation or gender identity\",\"sex\",\"race\",\"nation of origin, citizenship, immigrant status\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"information and communication\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"no tangible harm, near-miss, or issue\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "false"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy3\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"Natural Language Processesing\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"001\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"4. Peer review complete\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "14"
  }
  {
    "short_name": "Annotator",
    "value_json": "null"
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"Annotator 2: \\n\\n No tangible harm\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"Disproportionately gave phrases related to protected characteristics negative scores.\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"Disproportionately gave phrases related to protected characteristics negative scores.\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2017"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "10"
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "25"
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"Global\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Google\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Google Cloud Natural Language API\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Affected Groups\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Black people, Jewish people, LGBTQ+ people, and women were gauged as more inherently negative in sentiment analysis by the Google Cloud Natural Language API tool.\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Motherboard\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"unclear\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"watchdog\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Google\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Cloud Natural Language API\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Cloud Natural Language API Users \\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Disproportionately gave phrases related to protected characteristics negative scores.\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"Sentimental Analysis model. Given a phrase, the model outputs a score -1 to 1 which determines the sentiment of the phrase, negative or positive.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"text\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"natural language processing\",\"sentiment analysis\",\"Sentiment Analysis\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Taxonomy: CSETv1
Classification Count: 9

Based on the incident text and the taxonomy, provide a classification for this incident.

IMPORTANT: Your classification MUST include ALL of the following taxonomy attributes:
Incident Number, Annotator, Annotation Status, Peer Reviewer, Quality Control, Physical Objects, Entertainment Industry, Report, Test, or Study of data, Deployed, Producer Test in Controlled Conditions, Producer Test in Operational Conditions, User Test in Controlled Conditions, User Test in Operational Conditions, Harm Domain, Tangible Harm, AI System, Clear link to technology, There is a potentially identifiable specific entity that experienced the harm, AI Harm Level, AI Tangible Harm Level Notes, Impact on Critical Services, Rights Violation, Involving Minor, Detrimental Content, Protected Characteristic, Harm Distribution Basis, Notes (special interest intangible harm), Special Interest Intangible Harm, AI System, Clear link to Technology, Harmed Class of Entities, Annotator’s AI special interest intangible harm assessment, Notes (AI special interest intangible harm), Date of Incident Year, Date of Incident Month, Date of Incident Day, Estimated Date, Multiple AI Interaction, Embedded, Location City, Location State/Province (two letters), Location Country (two letters), Location Region, Infrastructure Sectors, Operating Conditions, Notes (Environmental and Temporal Characteristics), Entities, Lives Lost, Injuries, Estimated Harm Quantities, Notes ( Tangible Harm Quantities Information), AI System Description, Data Inputs, Sector of Deployment, Public Sector Deployment, Autonomy Level, Notes (Information about AI System), Intentional Harm, Physical System Type, AI Task, AI tools and methods, Notes (AI Functionality and Techniques)

For maximum accuracy and completeness:
1. Include EVERY single required field listed above in your response
2. Do not omit any attributes from the taxonomy field_list
3. Use the permitted_values from the taxonomy when provided
4. Review similar incidents to understand how each field is typically used

Return your response as a JSON object with the following structure:

{
  "classification": {
    "namespace": "CSETv1",
    "attributes": [
      {"short_name": "attribute1", "value_json": ""value1""},
      {"short_name": "attribute2", "value_json": ""value2""},
      
    ]
  },
  "explanation": "A detailed explanation of your classification choices.",
  "confidence": "A confidence score between 0 and 1"
}
  
DO NOT include any other text in your response, nor any other characters. 
DO NOT start your response with ```json or ```
Ensure that each attribute in the field_list is included in your classification, even if you need to use a default or "unknown" value.
