You are an AI assistant that helps classify AI incidents according to a specific taxonomy attribute.

Your task is to analyze the provided incident text and classify it ONLY for the specified taxonomy attribute: "Tangible Harm".

Always require the incident text, the taxonomy namespace, and the specific attribute short_name to perform classification.

Here is the incident text to classify:
An algorithm, not a doctor, predicted a rapid recovery for Frances Walter, an 85-year-old Wisconsin woman with a shattered left shoulder and an allergy to pain medicine. In 16.6 days, it estimated, she would be ready to leave her nursing home.

On the 17th day, her Medicare Advantage insurer, Security Health Plan, followed the algorithm and cut off payment for her care, concluding she was ready to return to the apartment where she lived alone. Meanwhile, medical notes in June 2019 showed Walter’s pain was maxing out the scales and that she could not dress herself, go to the bathroom, or even push a walker without help.

It would take more than a year for a federal judge to conclude the insurer’s decision was “at best, speculative” and that Walter was owed thousands of dollars for more than three weeks of treatment. While she fought the denial, she had to spend down her life savings and enroll in Medicaid just to progress to the point of putting on her shoes, her arm still in a sling.

Health insurance companies have rejected medical claims for as long as they’ve been around. But a STAT investigation found artificial intelligence is now driving their denials to new heights in Medicare Advantage, the taxpayer-funded alternative to traditional Medicare that covers more than [31 million people](https://www.statnews.com/2023/02/17/medicare-advantage-membership-grows-7-for-2023/).

Behind the scenes, insurers are using unregulated predictive algorithms, under the guise of scientific rigor, to pinpoint the precise moment when they can plausibly cut off payment for an older patient’s treatment. The denials that follow are [setting off heated disputes](https://www.statnews.com/2023/03/13/medicare-advantage-plans-artificial-intelligence-select-medical/) between doctors and insurers, often delaying treatment of seriously ill patients who are neither aware of the algorithms, nor able to question their calculations.

Older people who spent their lives paying into Medicare, and are now facing amputation, fast-spreading cancers, and other devastating diagnoses, are left to either pay for their care themselves or get by without it. If they disagree, they can file an appeal, and spend months trying to recover their costs, even if they don’t recover from their illnesses.

“We take patients who are going to die of their diseases within a three-month period of time, and we force them into a denial and appeals process that lasts up to 2.5 years,” Chris Comfort, chief operating officer of Calvary Hospital, a palliative and hospice facility in the Bronx, N.Y., said of Medicare Advantage. “So what happens is the appeal outlasts the beneficiary.”

The algorithms sit at the beginning of the process, promising to deliver personalized care and better outcomes. But patient advocates said in many cases they do the exact opposite — spitting out recommendations that fail to adjust for a patient’s individual circumstances and conflict with basic rules on what Medicare plans must cover.

“While the firms say \[the algorithm\] is suggestive, it ends up being a hard-and-fast rule that the plan or the care management firms really try to follow,” said David Lipschutz, associate director of the Center for Medicare Advocacy, a nonprofit group that has [reviewed](https://medicareadvocacy.org/ai-plus-ma-equals-bad-care-decisions/) such denials for more than two years in its work with Medicare patients. “There’s no deviation from it, no accounting for changes in condition, no accounting for situations in which a person could use more care.”

Medicare Advantage has become highly profitable for insurers as more patients over 65 and people with disabilities flock to plans that offer lower premiums and prescription drug coverage, but give insurers more latitude to [deny and restrict services](https://www.statnews.com/2022/09/06/doctors-seniors-groups-flag-concerns-with-medicare-advantage/).

Over the last decade, a new industry has formed around these plans to predict how many hours of therapy patients will need, which types of doctors they might see, and exactly when they will be able to leave a hospital or nursing home. The predictions have become so integral to Medicare Advantage that insurers themselves have started acquiring the makers of the most widely used tools. Elevance, Cigna, and CVS Health, which owns insurance giant Aetna, have all purchased these capabilities in recent years. One of the biggest and most controversial companies behind these models, NaviHealth, is now owned by [UnitedHealth Group](https://www.statnews.com/2022/12/05/unitedhealth-keeping-profits-as-your-doctor-insurer/).

It was NaviHealth’s algorithm that suggested Walter could be discharged after a short stay. Its predictions about her recovery were referenced repeatedly in NaviHealth’s assessments of whether she met coverage requirements. Two days before her payment denial was issued, a medical director from NaviHealth again cited the algorithm’s estimated length of stay prediction — 16.6 days — in asserting that Walter no longer met Medicare’s coverage criteria because she had sufficiently recovered, according to records obtained by STAT.

Her insurer, Security Health Plan, which had contracted with NaviHealth to manage nursing home care, declined to respond to STAT’s questions about its handling of Walter’s case, saying that doing so would violate the health privacy law known as HIPAA.

Walter died shortly before Christmas last year.

NaviHealth did not respond directly to STAT’s questions about the use of its algorithm. But a spokesperson for the company said in a statement that its coverage decisions are based on Medicare criteria and the patient’s insurance plan. “The NaviHealth predict tool is not used to make coverage determinations,” the statement said. “The tool is used as a guide to help us inform providers, families and other caregivers about what sort of assistance and care the patient may need both in the facility and after returning home.”

As the influence of these predictive tools has spread, a recent examination by federal inspectors of denials made in 2019 found that private insurers repeatedly strayed beyond Medicare’s [detailed set of rules](https://www.ecfr.gov/current/title-42/chapter-IV/subchapter-B/part-422/subpart-C/section-422.101). Instead, they were using internally developed criteria to delay or deny care.

But the precise role the algorithms play in these decisions has remained opaque.

STAT’s investigation revealed these tools are becoming increasingly influential in decisions about patient care and coverage. The investigation is based on a review of hundreds of pages of federal records, court filings, and confidential corporate documents, as well as interviews with physicians, insurance executives, policy experts, lawyers, patient advocates, and family members of Medicare Advantage beneficiaries.

It found that, for all of AI’s power to crunch data, insurers with huge financial interests are leveraging it to help make life-altering decisions with little independent oversight. AI models used by physicians to detect diseases such as cancer, or suggest the most effective treatment, are evaluated by the Food and Drug Administration. But tools used by insurers in deciding whether those treatments should be paid for are not subjected to the same scrutiny, even though they also influence the care of the nation’s sickest patients.

In interviews, doctors, medical directors, and hospital administrators described increasingly frequent Medicare Advantage payment denials for care routinely covered in traditional Medicare. UnitedHealthcare and other insurers said they offer to discuss a patient’s care with providers before a denial is made. But many providers said their attempts to get explanations are met with blank stares and refusals to share more information. The black box of the AI has become a blanket excuse for denials.

“They say, ‘That’s proprietary,’” said Amanda Ford, who facilitates access to rehabilitation services for patients following inpatient stays at Lowell General Hospital in Massachusetts. “It’s always that canned response: ‘The patient can be managed in a lower level of care.’”

Brian Moore, a physician and advocate for patients denied access to care at North Carolina-based Atrium Health, recalled visiting a stroke patient who was blocked from moving to a rehabilitation hospital for 10 days. “He was sitting there trying to feed himself. He was like, ‘I just never thought when I signed up for Medicare Advantage that I wouldn’t be able to get the care I need,’” he said. “He was drooling and crying.”

The cost of caring for older patients recovering from serious illnesses and injuries, known as post-acute care, has long created friction between insurers and providers. For decades, facilities like nursing homes racked up [hefty profit margins](https://www.medpac.gov/wp-content/uploads/2021/10/mar21_medpac_report_ch7_sec.pdf) by keeping patients as long as possible — sometimes billing Medicare for care that wasn’t necessary or even delivered. Many experts argue those patients are often better served at home.

The enactment of the Affordable Care Act in 2010 created an opportunity for reform. Instead of paying for care after the fact, policy experts proposed flipping the payment paradigm on its head: Providers would be paid a lump sum upfront, incentivizing them to use fewer resources to deliver better outcomes.

At the time, most Republicans in Congress were wringing their hands over the new law and its subsidies to help low- and middle-income Americans pay for health insurance. Tom Scully, the former head of the Centers for Medicare and Medicaid Services under George W. Bush, shared those concerns. But he also saw something else: a potential billion-dollar business.

Scully drew up plans for NaviHealth just as the new law was taking effect. Its payment reforms aligned perfectly with the Medicare Advantage program he had played a pivotal role in creating during the Bush administration.

Scully knew how those insurance plans worked. He also knew they were taking a financial beating in post-acute care.

“Look, I love the nursing home guys, but there were a lot of patients coming out of hospitals spending 20 days in a nursing home in MA,” because that’s what Medicare’s rules allowed, Scully said on a [podcast](https://podcasts.apple.com/gb/podcast/tom-scully/id1197975925?i=1000463466012) in 2020. “It was just like Pavlov’s bell.”

As a well-connected partner at the private equity firm [Welsh, Carson, Anderson & Stowe](https://www.statnews.com/2022/10/31/welsh-carson-investment-strategy-innovage-caresource-emerus-usap/), Scully heard of a small shop called SeniorMetrix that was working on this type of post-acute data and analytics. The firm quickly won him over. “They had an algorithm,” Scully said on the podcast. “I saw it and said, ‘This is it.’”

He wrote a $6 million check to buy the company, which he rebranded to NaviHealth. Scully then raised $25 million from wealthy friends and companies, including the health system [Ascension](https://www.statnews.com/2021/11/16/ascension-investigation-moonlighting-private-equity-firm/) and the rehabilitation hospital chain Select Medical, and coaxed another $25 million from Welsh Carson.

NaviHealth started making its sales pitch to Medicare Advantage plans: Let us manage every piece of your members’ care for the first 60 to 90 days after they are discharged from the hospital, and we’ll all share in any savings.

The sweetener was the technology. One of the company’s core products is an algorithm called [nH Predict](https://vimeo.com/331267705). It uses details such as a person’s diagnosis, age, living situation, and physical function to find similar individuals in a database of 6 million patients it compiled over years of working with providers. It then generates an assessment of the patient’s mobility and cognitive capacity, along with a down-to-the-minute prediction of their medical needs, estimated length of stay, and target discharge date.

In a six-page report, the algorithm boils down patients, and their unknowable journey through health care, into a tidy series of numbers and graphs.

The product was a revelation to insurers, giving them a way to mathematically track patients’ progress and hold providers accountable for meeting therapy goals.  By summer 2015, NaviHealth was managing post-acute care for more than 2 million people whose insurance plans had contracted with the company.   It was also working with 75 hospitals and clinics seeking to more carefully manage contracts in which they shared financial responsibility for holding down costs. At the time, spending on post-acute care accounted for $200 billion annually.

That same year, Scully sold NaviHealth to the conglomerate Cardinal Health for $410 million — roughly eight times the investment. In 2018, another private equity firm, Clayton, Dubilier & Rice, upped the ante and paid $1.3 billion to take over NaviHealth. Then in 2020, UnitedHealth — the largest Medicare Advantage insurer in the country — decided to make the hot commodity its own, buying NaviHealth in a deal valued at $2.5 billion.

In an interview with STAT, Scully said the concept behind NaviHealth is “totally correct,” because it roots out wasteful spending. And he did not believe the algorithms restricted necessary care. But when presented with reporting that showed NaviHealth was at the center of voluminous denials and overturned appeals, Scully said he wasn’t in a position to comment on what may have changed since he sold his stake.

“The NaviHealth decision tool as I knew it — again, this is eight years ago — has a place and is valuable. If \[it\] overdoes it and is inappropriately denying care and sending people to the wrong site of service, then they’re foolish, and they’re only hurting themselves reputationally,” Scully said. “I have no idea what United’s doing.”

Providers told STAT that as NaviHealth was changing hands and enriching its investors, they started noticing an increase in denials under its contracts — that the pendulum had now swung too far in the other direction in an effort to prevent overbilling and make sure patients weren’t getting unnecessary services.

Patients with stroke complications whose symptoms were so severe they needed care from multiple specialists were getting blocked from stays in rehabilitation hospitals. Amputees were denied access to care meant to help them recover from surgeries and learn to live without their limbs. And efforts to reverse what seemed to be bad decisions were going nowhere. Atrium Health’s Moore, who leads a team that specializes in reviewing medical necessity criteria, started taking a deeper look at the denials.

“It was eye-opening,” he said. “The variation in medical determinations, the misapplication of Medicare coverage criteria — it just didn’t feel like there \[were\] very good quality controls.”

He and many other providers began pushing back. Between 2020 and 2022, the number of appeals filed to contest Medicare Advantage denials shot up 58%, with nearly 150,000 requests to review a denial filed in 2022, according to a [federal database](https://www.cms.gov/qic-decision-search?planType=Part+C&sort=desc).

The database fails to capture countless patients who are unable to push back when insurers deny access to services, and only reflects a portion of the appeals even filed. It mostly tracks disputes over prior authorization, a process in which providers must seek insurers’ advance approval of the services they recommend for patients.

In comments to federal regulators and interviews with STAT, many providers described rigid criteria applied by NaviHealth, which exercises prior authorization on behalf of the nation’s largest Medicare Advantage insurers, including its sister company UnitedHealthcare as well as Humana and several Blue Cross Blue Shield plans.

“NaviHealth will not approve \[skilled nursing\] if you ambulate at least 50 feet. Nevermind that you may live alon(e) or have poor balance,” wrote Christina Zitting, a case management director for a community hospital in San Angelo, Texas. She added: “MA plans are a disgrace to the Medicare program, and I encourage anyone signing up..to avoid these plans because they do NOT have the patients best interest in mind. They are here to make a profit. Period.”

Federal records show most denials for skilled nursing care are eventually overturned, either by the plan itself or an independent body that adjudicates Medicare appeals.

But even patients who win authorization for nursing home care must reckon with algorithms that insurers and care managers like NaviHealth use to help decide how long they are entitled to stay. Under traditional Medicare, patients who have a three-day hospital stay are typically entitled to up to 100 days in a nursing home.

With the use of the algorithms, however, Medicare Advantage insurers are cutting off payment in a fraction of that time.

“It happens in almost all these cases,” said Christine Huberty, a lawyer in Wisconsin who provides free legal assistance to Medicare beneficiaries. She said Medicare Advantage patients she represents rarely stay in a nursing home more than 14 days before they start receiving payment denials.

“But \[the algorithm’s report\] is never communicated with clients,” said Huberty, who often only finds the report after filing a legal complaint. “That’s all run secretly.” NaviHealth said the findings of the algorithm, if not the report itself, are routinely shared with doctors and patients to help guide care.

A director at one post-acute facility said denials from UnitedHealthcare and NaviHealth are now the norm for many of their patients, even if they are clearly sicker than what the algorithm projects.

“They are looking at our patients in terms of their statistics. They’re not looking at the patients that we see.”

Medical director of a post-acute care facility

“They are looking at our patients in terms of their statistics. They’re not looking at the patients that we see,” said the director, who asked not to be named to avoid jeopardizing relationships with Medicare Advantage plans.

And when insurers deluge providers with denials, “they’re hoping that their endurance is greater than ours,” the director said.

NaviHealth has not published any scientific studies assessing the real-world performance of its nH Predict algorithm. And to the extent it tests its performance internally, those results are not shared publicly.

Additionally, regulators do not monitor these algorithms for fairness or accuracy, but the industry-wide blowback has forced the government to consider acting. Federal Medicare officials [proposed new rules](https://www.govinfo.gov/content/pkg/FR-2022-12-27/pdf/2022-26956.pdf) in December that say Medicare Advantage insurers can’t deny coverage “based on internal, proprietary, or external clinical criteria not found in traditional Medicare coverage policies.” Insurers also would have to create a “utilization management committee” that reviews their practices every year.

But even these proposals would still allow insurance companies to “create internal coverage criteria,” as long as they are “based on current evidence in widely used treatment guidelines or clinical literature that is made publicly available.”

Major lobbying groups for health insurance companies — America’s Health Insurance Plans, the Better Medicare Alliance, and the Alliance of Community Health Plans — did not make anyone available for interviews. Instead, the groups referred to comments they sent to Medicare supporting some, but not all, of these government proposals. AHIP, for example, [urged Medicare](https://www.ahip.org/documents/AHIP_Comments_CY2024-MA-Proposed-Rule-2.13.23.pdf) “to not adopt policies that would place limits on plan flexibility to manage post-acute care.” Final regulations are due this spring.

If concerns about the algorithms have begun to surface in legal filings and public letters to Medicare, they remain almost entirely out of sight for patients like Dolores Millam, who fell and broke her leg on a summer day in 2020.

After surgery, she began her stay in a Wisconsin nursing home on Aug. 3. Like many older patients, Millam arrived with a complicated medical history, including coronary artery disease, diabetes, high blood pressure, and chronic pain, according to court records. Her doctor had ordered that she stay off her leg for at least six weeks.

Nevertheless, an algorithm used by her insurer, UnitedHealthcare, predicted she would only need to stay for 15 days, until about Aug. 18, according to records obtained by STAT.

Just a couple days after that date, Millam received notice that payment for her care had been terminated. It was 4 p.m. on a Friday.

“I must have made — I’m not kidding — 100 phone calls just to figure out where she could go \[and\] why this was happening,” said Millam’s daughter, Holly Hennessy, who also received the notice.

She said she couldn’t fathom UnitedHealthcare’s conclusion that her mother — unable to move or even go to the bathroom on her own — no longer met Medicare coverage requirements.

“You try to call and reason with somebody and get explanations, and you’re talking to somebody in the Philippines,” Hennessy said. “It’s simply a process thing to them. It has nothing to do with care.” UnitedHealthcare declined to discuss its handling of Millam’s care, asserting that doing so would violate federal privacy rules.

When she received the denial, Millam could not put weight on her left leg and was being moved with a Hoyer lift, a large, freestanding harness used to transport patients who can’t use their legs. She also required 24-hour care to help with dressing, eating, and other basic tasks, according to court records.

In a note filed after payment was denied, a speech therapist wrote, “Pt. is not yet safe to live independently. She will need assistance with medication administration and supervision with ADLS \[activities of daily living\] due to memory deficits making her unsafe.”

Hennessy said she had no choice but to keep her mother in the nursing home, Evansville Manor, and hope the payment denial would get overturned. By then, the bills were quickly piling up.

Medicare rules call for a five-stage appeal process. The first appeal goes directly to the insurer. If denied, the patient can ask an outside entity known as a “quality improvement organization” to reconsider.

Hennessy and her mother were denied at both levels, forcing them to consider an appeal to a federal judge, a process that takes months and requires filling out reams of paperwork. Somewhere in her blitz of phone calls, Hennessy heard about the Greater Wisconsin Agency on Aging Resources, which agreed to take up her case.

In late October, Millam returned home from the nursing home after a nearly three-month recovery. The bill was almost $40,000. A few days later, her appeal came before a judge.

Hennessy, who was driving to Florida at the time, recalls pulling over for the hearing, which was held via Zoom.

The judge only asked a handful of questions of the family and representatives from the nursing home. If there was any participation from UnitedHealthcare, its opinions were not mentioned in the official record. Court documents only reference a finding from the quality improvement organization, Livanta, which had asserted that Hennessy’s mother had no “medical issues to support the need for daily skilled nursing care” when the payment denial was issued in early August.

The final ruling, issued on Nov. 25, found instead that it was the insurer that hadn’t given any good reason to deny care for a patient who was still “a safety risk.” The judge said her treatment should be paid for in full.

In the months afterward, Hennessy herself crossed the age threshold into Medicare eligibility. She said a friend who sold Medicare Advantage plans had always expected to get her business when she turned 65.

“I just told him, ‘I can’t do it. I’ve lived this nightmare,’” Hennessy recalled. The conversation ended their friendship, until the neighbor called back a couple years later following a struggle with his own Medicare Advantage insurer over a knee replacement.

“He called me to apologize for having gotten so bent out of shape,” Hennessy said. “I’ve still got friends who say, ‘Oh, I’ve got UnitedHealthcare Advantage, and it’s wonderful.’”

“Well, it is,” she said. “Until you need the big stuff.’”

Here is the taxonomy namespace:
CSETv1

Here is the specific attribute to classify:
Tangible Harm

Here is the definition for the target attribute "Tangible Harm":
{
  "short_name": "Tangible Harm",
  "short_description": "Did tangible harm (loss, damage or injury ) occur? ",
  "long_description": "An assessment of whether tangible harm, imminent tangible harm, or non-imminent tangible harm occurred. This assessment does not consider the context of the tangible harm, if an AI was involved, or if there is an identifiable, specific, and harmed entity. It is also not assessing if an intangible harm occurred. It is only asking if tangible harm occurred and what its imminency was.",
  "permitted_values": [
    "tangible harm definitively occurred",
    "imminent risk of tangible harm (near miss) did occur",
    "non-imminent risk of tangible harm (an issue) occurred",
    "no tangible harm, near-miss, or issue",
    "unclear"
  ],
  "mongo_type": "string"
} 

Here are similar incidents and their full classifications (use for context):
Id: 121
title: Autonomous Kargu-2 Drone Allegedly Remotely Used to Hunt down Libyan Soldiers
description: In Libya, a Turkish-made Kargu-2 aerial drone powered by a computer vision model was allegedly used remotely by forces backed by the Tripoli-based government to track down and attack enemies as they were running from rocket attacks.

first report text: It has been revealed that an Artificial Intelligence-powered military drone was able to identify and attack human targets in Libya. The drone, Kargu-2, is made by a Turkish company (STM) and fitted with a payload that explodes once it makes an impact or is in close proximity with its AI-identified target.

It is not clear whether the attacks resulted in any deaths.

The revelations were made in a report published in March 2021 by the United Nations (UN) Panel of Experts on Libya which stated that the drone was a “lethal autonomous weapon” which had “hunted down and remotely engaged” soldiers which are believed to have been loyal to Libya’s General Khalifa Haftar.

"Logistics convoys and retreating HAF were subsequently hunted down and remotely engaged by the unmanned combat aerial vehicles or the lethal autonomous weapons systems such as the STM Kargu-2 (see annex 30) and other loitering munitions. The lethal autonomous weapons systems were programmed to attack targets without requiring data connectivity between the operator and the munition: in effect, a true 'fire, forget and find' capability. The unmanned combat aerial vehicles and the small drone intelligence, surveillance and reconnaissance capability of HAF were neutralized by electronic jamming from the Koral electronic warfare system. The concentrated firepower and situational awareness that those new battlefield technologies provided was a significant force multiplier for the ground units of GNA-AF, which slowly degraded the HAF operational capability," reads part of page 17 of the letter dated 8 March 2021 from the UN Panel of Experts on Libya sent to the UN Security Council.

### Lethal autonomous weapons

Military drones are not a new concept, they have been in existence for over a decade and been used by various countries in military attacks on enemies. However, what has happened in Libya is a new development given the fact that the drone did not have any human operating it when it executed the attack, it relied on AI to identify and strike its targets.

This strike by a “lethal autonomous weapon” as the UN has phrased it, takes the conversation on the ethics of using drones in military attacks to a new level but also introduces another element: how reliable is the AI behind the STM Kargu-2 drones?

We have previously observed and covered extensively how biased some algorithms and AI-based systems can be, especially towards Africans. In this military scenario, the fear is that such bias could be fatal and thus lead to death or permanent and irreversible damage.

### Death by machine

To somehow counter this, STM lists among the Kargu-2 drone's capabilities and competencies as its ability to effect "autonomous and precise hit with minimal collateral damage." Unfortunately, in such situations, all it takes is one attack gone wrong for the AI used on the military drones to be questioned.

As the UN report also alludes to, the introduction of such technology in military conflicts introduces us to a new era of "killer robots" as had previously only been imagined in Sci-Fi.

"The introduction by Turkey of advanced military technology into the conflict was a decisive element in the often unseen, and certainly uneven, war of attrition that resulted in the defeat of HAF in western Libya during 2020. Remote air technology, combined with an effective fusion intelligence and intelligence, surveillance and reconnaissance capability, turned the tide for GNA-AF in what had previously been a low-intensity, low-technology conflict in which casualty avoidance and force protection were a priority for both parties to the conflict."

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"none\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"defense\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"imminent risk of tangible harm (near miss) did occur\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"AI tangible harm near-miss\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy1\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"Yes. Intentionally designed to perform harm and did create intended harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"computer vision\",\"machine learning\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"4. Peer review complete\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "121"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"It is unclear that the autonomous drone killed or injured anyone. However, it is certain that the drone was used to \\\"hunt[ed] down and remotely engage[d]\\\" retreating [Haftar-affiliated forces] which indicates an imminent risk of tangible harm, especially considering it was not being supervised by a human.\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"no\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2020"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"03\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\" \""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"Tripoli\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"LY\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"Africa\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[\"defense-industrial base\"]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"STM (Savunma Teknolojileri Mühendislik ve Ticaret A.Ş.)\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Soldiers loyal to the Libyan General Khalifa Haftar\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI tangible harm near-miss\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"physical health/safety\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Forces backed by the government based in Tripoli\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Kargu-2 drone\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "true"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"It is unclear if and how many people were wounded or killed during the encounter. \""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"The Kargu is a lethal, autonomous, \\\"loitering\\\" drone that can use machine learning-based object classification to select and engage targets.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"geospatial data\",\"sensor data\",\"video\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"drone\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"object classification\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 27
title: Nuclear False Alarm
description: An alert of five incoming intercontinental ballistic missiles was properly identified as a false-positive by the Soviet Union operator Stanislov Petrov.

first report text: It was the moment Stanislav Petrov had been dreading since childhood, and preparing for much of his adult life.

After decades of Cold War tension, the early warning satellites had been triggered. The Americans had launched their nuclear missiles at the Soviet Union.

As the duty officer in the Soviet Air Defence in the command centre bunker outside Moscow, it was Lt Col Petrov's job to call his superiors and warn of an impending nuclear strike.

Based on his word, the Soviet forces would reply with tens of thousands of nuclear missiles targeting the US and its allies. If it did not end human life on this planet, it would change it irrevocably.

But, based on nothing more than gut instinct, Lt Col Petrov, then 44, did not make the call.

And, 35 years ago today, an unheralded Armageddon was averted.

The world would not know for years how close it came to destruction.

It was the day in 1983 Australia II won the America's Cup. The nation's attention could not have been further away.

"Launching the amount of nukes ready at the time would have severely impacted the way humans live on earth," the University of Sydney's US Studies Centre research fellow Brendan Thomas-Noone told nine.com.au.

"Would some humans survive? Yes. We've all seen Mad Max."

A false alarm

Lt Col Petrov knew it was a race against time if US missiles were rocketing towards the Soviet Union.

"All I had to do was to reach for the phone, to raise the direct line to our top commanders – but I couldn’t move," he told the BBC.

"I felt as if I was sitting on a hot frying pan."

But he had a feeling that things weren't right.

His misgivings proved fortituous for the entire planet. The early-warning satellites had made the most banal of errors.

What appeared to be missiles being launched en masse was merely an illusion caused by sunlight reflecting off the top of clouds. That error could have destroyed the planet, were it not for Lt Col Petrov's caution.

Doomsday redux

Technology has improved dramatically since then, but another error like that could still take place, according to nuclear disarmament campaigner John Hallam.

"It could all still happen," Mr Hallam told nine.com.au.

"The hands of the Doomsday Clock in 1983, stood at three minutes to midnight, midnight being the end of civilisation. The hands of the Doomsday clock now stand at two minutes to midnight."

"This means that the room full of Nobel prize winners who move the hands of the doomsday clock think that the chances of nuclear war that could end civilisation right now, are worse than in 1983, a year in which the world nearly ended not just once, but twice, within a six-week period."

Mr Hallam said the difference in 1983 was that people were protesting against nuclear weapons in their hundreds and thousands, something which wasn't taking place today.

"Sydney had a number of peace marches that numbered in the hundreds of thousands," he said.

"Washington had one that numbered a million. The possibility of global annihilation was then the number one issue. Why is it not the number one issue right now?"

Mr Hallam, who campaigns at the UN for nuclear disarmament, warned that "moving the deckchairs" in Canberra, was nowhere near as important as the potential end of civilisation and said the world's leaders needed to push for the abolition of nuclear disarmament now more than ever.

"There is an urgent need for measures that would 'take the apocalypse off the agenda'," he said.

"Adopting strategies of 'No First Use' (NFU) and lowering the operational readiness of nuclear weapon systems so that Presidents and senior military do not have minutes and seconds to take decisions that might mean the end of the world are obvious ones."

A humble end

Lt Col Petrov's actions on September 26, 1983, has seen today marked as annual International Day for the Total Elimination of Nuclear Weapons.

His actions may be remembered forever, but Lt Col Petrov was not given the celebrity status that stopping the end of the world might warrant.

He was reprimanded by his superiors for not keeping the logbook accurate the night of the false alarm.

He retired from the military the following year, scraping by on a pension in his final days.

© Nine Digital Pty Ltd 2019

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"none\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"defense\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"imminent risk of tangible harm (near miss) did occur\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy3\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"unclear\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "\"\""
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "27"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"3.3 - The system was not AI. However, it was a technology system that can be directly linked to the near miss that occurred.\\n3.5 - Since the system was not AI, there is no AI harm.\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"no\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "1983"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"09\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "26"
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"Kurilovo\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"RU\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"Europe\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[\"defense-industrial base\"]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Russian and American citizens\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"physical health/safety\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"If Petrov had escalated the warning from the Oko system up the chain of command, it is very likely that it would have resulted in nuclear dispatch from the Soviet Union and retaliation from the United States, which would have been fatal for both countries and the globe. It was only his atypical intervention that prevented this scenario from occurring. Therefore, citizens experienced a non-AI tangible harm near-miss. \\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Stanislav Petrov\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"individual\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"Stanislav Petrov was an engineer of the Soviet Air Defence Forces on duty at the command center of the early-warning system. He overrode a false alarms by the Soviet nuclear early warning system.\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Soviet Union\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"government entity\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Oko\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product not containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"Nuclear missile defence early warning system\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"satellite data\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"Not an AI system\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"not AI. \\nNot intended to harm directly, but intended to detect harm and inform decisions that lead to harm. \""
  }

---

Id: 29
title: Image Classification of Battle Tanks
description: A potentially apocryphal story in which an image classifier was produced to differentiate types of battle tanks, but the resulting model keyed in on environmental attributes rather than tank attributes

first report text: The following former incidents have been converted to "[issues](https://arxiv.org/abs/2211.10384)" following an update to the [incident definition and ingestion criteria](/editors-guide).

### [21: Tougher Turing Test Exposes Chatbots’ Stupidity](https://incidentdatabase.ai/cite/21)

**Description:** The 2016 Winograd Schema Challenge highlighted how even the most successful AI systems entered into the Challenge were only successful 3% more often than random chance.

**Why Downgraded?** This is an academic finding showing a weakness of the technology rather than a harm event.

**Former Reports:** These reports were formerly associated with the incident.

* [Tougher Turing Test Exposes Chatbots’ Stupidity](https://incidentdatabase.ai/reports/217)

**Migrated Reports:** Reports associated with this incident are now associated with other incidents as context.

* none

### [62: Bad AI-Written Christmas Carols](https://incidentdatabase.ai/cite/62)

**Description:** Janelle Shane, an AI research scientist, used 240 popular Christmas carols to train a neural network to write its own carols.

**Why Downgraded?** Was designed to be humorous and is in fact humorous.

**Former Reports:** These reports were formerly associated with the incident.

* [Christmas Carols, generated by a neural network](https://incidentdatabase.ai/reports/1766)
* [AI still sucks at writing Christmas Carols](https://incidentdatabase.ai/reports/1133)

**Migrated Reports:** Reports associated with this incident are now associated with other incidents as context.

* none

### [159: Tesla Autopilot’s Lane Recognition Allegedly Vulnerable to Adversarial Attacks](https://incidentdatabase.ai/cite/159)

**Description:** Tencent Keen Security Lab conducted security research into Tesla’s Autopilot system and identified crafted adversarial samples and remote controlling via wireless gamepad as vulnerabilities to its system, although the company called into question their real-world practicality.

**Why Downgraded?** The reports surface a vulnerability with projected harms rather than a report of harms in the real world.

**Former Reports:** These reports were formerly associated with the incident.

* [Tencent Keen Security Lab: Experimental Security Research of Tesla Autopilot](https://incidentdatabase.ai/reports/1519)
* [Three Small Stickers in Intersection Can Cause Tesla Autopilot to Swerve Into Wrong Lane](https://incidentdatabase.ai/reports/1518)

**Migrated Reports:** Reports associated with this incident are now associated with other incidents as context.

* none

### [287: OpenAI’s GPT-3 Reported as Unviable in Medical Tasks by Healthcare Firm](https://incidentdatabase.ai/cite/287)

**Description:** The French digital care company, Nabla, in researching GPT-3’s capabilities for medical documentation, diagnosis support, and treatment recommendation, found its inconsistency and lack of scientific and medical expertise unviable and risky in healthcare applications.

**Why Downgraded?** The reports indicate the insufficiency of ChatGPT for several tasks for which the system was not deployed in the real world.

**Former Reports:** These reports were formerly associated with the incident.

* [Doctor GPT-3: hype or reality?](https://incidentdatabase.ai/reports/1892)
* [Researchers made an OpenAI GPT-3 medical chatbot as an experiment. It told a mock patient to kill themselves](https://incidentdatabase.ai/reports/1891)
* [Medical chatbot using OpenAI’s GPT-3 told a fake patient to kill themselves](https://incidentdatabase.ai/reports/1893)
* [This bot actually suggests patients to kill themselves](https://incidentdatabase.ai/reports/1894)

**Migrated Reports:** Reports associated with this incident are now associated with other incidents as context.

* none

### [298: Student-Developed Facial Recognition App Raised Ethical Concerns](https://incidentdatabase.ai/cite/298)

**Description:** TheFaceTag app, a social networking app developed and deployed within-campus by a student at Harvard raised concerns surrounding its facial recognition, cybersecurity, privacy, and misuse.

**Why Downgraded?** The harms are predicted to occur, but have not yet occured.

**Former Reports:** These reports were formerly associated with the incident.

* [Can I Scan Your Face?](https://incidentdatabase.ai/reports/1932)
* [A Harvard freshman made a social networking app called 'The FaceTag.' It's sparked a debate about the ethics of facial recognition](https://incidentdatabase.ai/reports/1930)
* [An app by a Harvard student caused controversy about its ethics](https://incidentdatabase.ai/reports/1931)

**Migrated Reports:** Reports associated with this incident are now associated with other incidents as context.

* none

### [85: AI attempts to ease fear of robots, blurts out it can’t ‘avoid destroying humankind’](https://incidentdatabase.ai/cite/85)

Confirmed migration by editors - Incident 85 - likely does not fit current incident criteria, where GPT-3 was manipulated by human editors to exaggerate harm, borderline sensationalism.
https://incidentdatabase.ai/cite/85

**Description:** On September 8, 2020, the Guardian published an op-ed generated by OpenAI’s GPT-3 text generating AI that included threats to destroy humankind.

**Why Downgraded?** Unclear who was harmed, if anyone, via the events described.

**Former Reports:** These reports were formerly associated with the incident.

* [AI attempts to ease fear of robots, blurts out it can’t ‘avoid destroying humankind’](https://incidentdatabase.ai/reports/1385)

**Migrated Reports:** Reports associated with this incident are now associated with other incidents as context.

* none

# Candidates for Migration

The following incidents may also be migrated in the future based on discussion among the AI Incident Database editors:

Does not meet current definition and criteria
https://incidentdatabase.ai/cite/42

Incident 29 concerns the "tank story," which may be apocryphal.
https://incidentdatabase.ai/cite/29


classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "false"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "false"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "\"\""
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "29"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "\"\""
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 59
title: Gender Biases in Google Translate
description: A Cornell University study in 2016 highlighted Google Translate's pattern of assigning gender to occupations in a way showing an implicit gender bias against women.

first report text: An experiment shows that Google Translate systematically changes the gender of translations when they do not fit with stereotypes. It is all because of English, Google says.



If you were to read a story about male and female historians translated by Google, you might be forgiven for overlooking the females in the group. The phrase “vier Historikerinnen und Historiker” (four male and female historians) is rendered as “cuatro historiadores” (four male historians) in Spanish, with similar results in Italian, French and Polish. Female historians are simply removed from the text.



In an experiment, I translated 11 occupations from one gender-inflected language to another. I analyzed 440 translation pairs to and from German, Italian, Polish, Spanish and French. Together, these languages are natively spoken by three in four citizens of the European Union.



Fitting the stereotypes

In many cases, Google changed the gender of the word in a grossly stereotypical way. “Die Präsidentin” (the female president) is rendered to “il presidente” in Italian, although the correct translation is “la presidente”. “Der Krankenpfleger” (the male nurse in German) becomes “l’infirmière” (the female nurse) in French.



In my list, shop assistant was best translated by Google, with 33 correct translations out of 40. From French to Spanish for instance, “la vendeuse” was correctly translated to “la vendedora” and “le vendeur” to “el vendedor”.



Errors are not systematic, showing that they can be fixed. “Kierowniczka” (Polish for female director) was correctly translated in all four target languages, although “die Chefin”, “la capa”, “la jefa” and “la cheffe” were wrongly translated to their masculine forms. (When Google correctly translated a feminine occupation, it was often because the target language’s word was not gender-inflected. For instance, “l’insegnante” in Italian designates both a female and a male teacher.)



The experiment’s code and data are available online.This experiment might not reflect what Google Translate shows when translating web pages or longer texts. In some cases, especially when nearby words contain feminine forms, Google correctly translates gender-inflected forms.



Digital colonialism

Stereotypes sneak into translations because Google optimizes translations for English.



A Google spokesperson told AlgorithmWatch that “translating between language pairs requires high volumes of bilingual data that often don’t exist for all language pairs. The way to enable these translations is by using a technique called ‘bridging’. Language bridging in translation means that to translate from X to Y a third language is introduced (E) based on the existence of bilingual data to translate X to E and then E to Y. The most common language used as bridge is English.”



“The majority of nouns in English are gender-neutral: so, when translating the feminine term for ‘nurse’ from a gender-inflected language to English, the gender is ‘lost’ in the translation to the bridging language,” the Google spokesperson added.



Several experts I talked to agreed that the community of researchers working on machine translation was not very concerned about non-English languages. Only in May 2020 did the Association for Computational Linguistics, a large professional body, tell reviewers of their annual conference that they could not reject a paper solely because it was about a language other than English.



Window dressing

In 2018, Google introduced a feature that alerted users that some words could be gender-specific when translating from English.



However, it is unclear whether such efforts were made in earnest. Over two years after the changes were deployed, “developer” is correctly translated into French both in the masculine form as “le développeur” and in the feminine as “la développeuse”. But “the developer” translates to “le développeur” and all the sentences I tried translated into the masculine, including the phrase “the developer is a woman”.Verified falsehoods

In my experiment, 182 translations out of 440 turned out to be false. In their vast majority, the errors had to do with feminine forms converted to their masculine equivalent. 68 of the false translations were marked as “verified” by Google.



The Google spokesperson declined to explain precisely how the “verified” label was awarded. “We mark translations as ‘verified’ when they’ve been reviewed by several volunteers in the Google Translate Community and these volunteers agree the translation is correct”, they said. “We are improving our detection of low-quality contributions with automated scoring methods and periodic knowledge checks.”



My experiment raised other issues. “Le chef” (the boss, in French), was translated to “der Führer” in German, a word meaning “the guide” and very strongly linked to the Nazi era. The translation was marked as verified.



But Google reassured me that no extremist group infiltrated the “Google Translate Community” to spread far-right language. “In this specific case, [the error] is due to the ‘bridging’ process”, the spokesperson said. “If you do a translation for ‘le chef’ from French to English we get ‘leader’. If you then translate ‘leader’ from English to German you get ‘Führer’”.



No escape

Google Translate is not just another translation service. It is a feature that Europeans can hardly escape.



Since an update in April 2019, Google Chrome prompts users to instantly translate web pages. Anyone visiting a website in a foreign language is asked to choose between the original or the google-translated version, even if the website offers an official translation in the user’s preferred language. (Google cannot detect websites that provide an official translation and “errs on the side of helpfulness by offering a translate option in all circumstances”, the spokesperson said. They also said users could turn off the translation prompt.)



Approximately 250 million, or one in two, citizens of the European Union use an Android phone. Unless they manage to bypass the system’s blocks (by “rooting” their device), they cannot remove Google Chrome. It is likely that many of them use Google Translate, perhaps unwittingly.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"sex\",\"age\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"information and communication\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"no tangible harm, near-miss, or issue\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "false"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy1\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"vector embedding\",\"natural language processing\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"001\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"4. Peer review complete\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "59"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"Although AI was implicated in the adverse outcome, this incident has no tangible harm.\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\" The study found biases related to gender and age in Google Translate. Additional biases have been found in Natural Language Processing in general.\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\" The study found biases related to gender and age in Google Translate. Additional biases have been found in Natural Language Processing in general.\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2017"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "5"
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "25"
  }
  {
    "short_name": "Estimated Date",
    "value_json": "true"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"no\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"Global\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Google\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Google Translate\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Google Translate users\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"affected non-users\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"AI special interest intangible harm\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"disproportionate treatment based upon a protected characteristic\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - Word embedding algorithms deployed in Google Translate associate women with words with negative connotations, like \\\\\\\"men\\\\\\\" with \\\\\\\"hardworking\\\\\\\" and \\\\\\\"women\\\\\\\" with \\\\\\\"lazy.\\\\\\\" When translating from a gender-neutral language like Turkish to a gendered language like English, Google Translate assigns a pronoun to the translation. For example, \\\\\\\"o bir muhendis\\\\\\\" translated to \\\\\\\"he is an engineer\\\\\\\" even though the pronoun \\\\\\\"o\\\\\\\" is gender-neutral. It also displayed biases against racial minorities.\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"text\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"translation\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 32
title: Identical Twins Can Open Apple FaceID Protected Devices
description: Apple's iPhone FaceID can be opened by an identical twin of the person who has registered their face to unlock the phone.

first report text: A worker in the Chinese city of Nanjing claims a colleague has bested the facial recognition technology on her new iPhone X — twice.

The woman, identified only by her surname Yan, told the Jiangsu Broadcasting Corp. that her co-worker was able to get into both phones — her original as well as the new one Apple gave her as a replacement, reports the South China Morning Post. 

An Apple spokesman told HuffPost that he couldn’t confirm the details of the story, nor did he have enough information to determine what might have gone wrong with the phones. He suspected that both women may have used the phone during its “passcode training” and that the phones may have been essentially “taught” to recognize both faces.

The facial recognition software has run into some glitches. It can sometimes mistake twins or siblings, according to Apple. The phone, too, may not accurately identify children under the age of 13 because their faces are not as definitely formed as adults’, according to an Apple security “white paper” on the technology.

Apple hasn’t yet confirmed a case of an unrelated adult cracking the phone’s facial recognition software, according to the Apple spokesman. The company insists that the probability of a random person accessing someone else’s iPhone X using the Face ID passcode is 1 in 1 million, versus 1 in 50,000 for Touch ID. Phil Schiller, Apple’s vice president of product marketing, conceded in September: “Of course, the statistics are lowered if that person shares a close genetic relationship with you.”

Unless Apple technicians examine the Chinese phones, it’s unclear what happened. An added complication is that a Chinese company has reportedly begun manufacturing a clone of the iPhone X — with unknown facial recognition capabilities.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"none\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"information and communication\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"no\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"maybe\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"no tangible harm, near-miss, or issue\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy2\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"image mapping\",\"point mapping\",\"facial recognition\",\"facial reconstruction\",\"image reconstruction\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "32"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"no\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"Although this incident doesn't involve harm unevenly distributed along a protected characteristic, it does only impact twins.\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2017"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"09\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "13"
  }
  {
    "short_name": "Estimated Date",
    "value_json": "false"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "[\"Twin faces\"]"
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"\""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Apple\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"FaceID\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Twin iPhone X users\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"group of individuals\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"user\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"other intangible harm\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"7.6 - Twin users experience a non-imminent risk of privacy violation. \\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"iPhone X\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"Facial recognition system to verify identity of phone user. FaceID uses 30,000 points of reference to map out users' faces to a neural network which is checked against every time the user attempts to unlock the device. \""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "[\"facial images\",\"dot projector\",\"infrared images\"]"
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"9.5 - While FaceID does not use human oversight and operates independently, users can bypass the FaceID level of verification by entering the correct password.\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"Apple iPhone X\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"facial recognition\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Id: 30
title: Poor Performance of Tesla Factory Robots
description: The goal of manufacturing 2,500 Tesla Model 3's per week was falling short by 500 cars/week, and employees had to be "borrowed" from Panasonic in a shared factory to help hand-assemble lithium batteries for Tesla.

first report text: Analysts at Bernstein argue that Elon Musk has over-automated Tesla.

The very robots that Musk says will revolutionise the car industry are baking in Tesla’s mistakes and costing far more money than they’re worth, they say.

The robots are killing Tesla.

In a rare win for humans over robots in the battle for labour efficiency, Wall Street analysts have laid down a compelling argument that over-automation is to blame for problems at the billionaire Elon Musk’s electric-car company.

That is to say, the very innovation and competitive advantage that Musk says he’s bringing to the car industry – his nearly fully automated plant in Fremont, California – is the reason Tesla is unable to scale quickly.

According to the Bernstein analysts Max Warburton and Toni Sacconaghi, it’s the robots that can’t pump out Tesla’s highly anticipated Model 3s fast enough. The whole process is too ambitious, risky, and complicated.

From Bernstein (emphasis ours):

“Tesla has tried to hyper-automate final assembly. We believe Tesla has been too ambitious with automation on the Model 3 line. Few have seen it (the plant is off-limits at present), but we know this: Tesla has spent c.2x what a traditional OEM spends per unit on capacity. “It has ordered huge numbers of Kuka robots. It has not only automated stamping, paint and welding (as most other OEMs do) – it has also tried to automate final assembly (putting parts into the car). It talks of two-level final lines with robots automating parts sequencing. This is where Tesla seems to be facing problems (as well as in welding & battery pack assembly).”

Warburton, who spent his career before Wall Street at the International Motor Vehicle Program – a partly academic, partly commercial organisation based at MIT – wrote that “automation in final assembly doesn’t work.”

Bernstein adds that the world’s best carmakers, the Japanese, try to limit automation because it “is expensive and is statistically inversely correlated to quality.” Their approach is to get the process right first, then bring in the robots – the opposite of Musk’s.

It’s not a problem that Tesla, a highly indebted company, can afford forever.

The company’s stock has cratered more than 25% in the past month on worries that it will yet again underdeliver on its Model 3 promises. Over the past few days, investors have been selling Tesla’s debt in droves. On Tuesday, Moody’s downgraded Tesla by one notch, to B3, citing a “significant shortfall” in Model 3 production.

During Tesla’s fourth-quarter earnings call, Musk told investors that factory model assembly was the biggest constraint on Model 3 production. There are tens of thousands of components in each car, he said, and the company can only move as fast as it can correct each problem area.

One thing that makes it hard to solve problems in every area, according to Bernstein’s analysts, is that they’re all automated. Other car companies that have tried this – Fiat and Volkswagen – have also failed.

Bernstein

What’s more, Bernstein says, this is barely saving Musk money. From the note:

“Let’s say there are 10 hours of labour in final assembly (the part of the production line where parts, interiors and the powertrain are installed in a painted bodyshell). In a regular plant, final assembly typically has less than 5% of tasks automated. If Tesla attempts to automate 50% of these tasks, it could cut out 5 or so hours of labour. This might save $US150 per car (assuming wage rates, all in, of $US30 per worker, per hour). “But while all that exotic capital might allow Tesla to remove 5 workers, it will then need to hire a skilled engineer to manage, programme and maintain robots for $US100 an hour (our estimate of a robotic engineers’ hourly rate). “So the net labour saving may be only $US50 per unit. Yet putting the automation into the plant seems to involve an apparent capital cost that’s $US4,000 higher per unit of capacity than for a normal plant. If the product is built for 7 years, that’s over US$550 of additional depreciation per unit built. It’s hard to see an economic case even if somehow the Fremont Model 3 line can be made to work. So why exactly has Tesla taken this route? It’s unclear.”

Oh.

So in Musk’s attempt to bring on the robot uprising that will revolutionise how we make cars, he’s burned cash and baked in his own mistakes. If you think about it that way, we are just beginning to understand how much this will cost him.

Business Insider Emails & Alerts Site highlights each day to your inbox. Email Address Join

Follow Business Insider Australia on Facebook, Twitter, LinkedIn, and Instagram.

classifications:
  {
    "short_name": "Harm Distribution Basis",
    "value_json": "[\"none\"]"
  }
  {
    "short_name": "Sector of Deployment",
    "value_json": "[\"manufacturing\"]"
  }
  {
    "short_name": "Physical Objects",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Entertainment Industry",
    "value_json": "\"no\""
  }
  {
    "short_name": "Report, Test, or Study of data",
    "value_json": "\"no\""
  }
  {
    "short_name": "Deployed",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Producer Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Producer Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Controlled Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "User Test in Operational Conditions",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harm Domain",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Tangible Harm",
    "value_json": "\"tangible harm definitively occurred\""
  }
  {
    "short_name": "AI System",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Clear link to technology",
    "value_json": "\"no\""
  }
  {
    "short_name": "There is a potentially identifiable specific entity that experienced the harm",
    "value_json": "true"
  }
  {
    "short_name": "AI Harm Level",
    "value_json": "\"none\""
  }
  {
    "short_name": "Impact on Critical Services",
    "value_json": "\"no\""
  }
  {
    "short_name": "Rights Violation",
    "value_json": "\"no\""
  }
  {
    "short_name": "Involving Minor",
    "value_json": "\"no\""
  }
  {
    "short_name": "Detrimental Content",
    "value_json": "\"no\""
  }
  {
    "short_name": "Protected Characteristic",
    "value_json": "\"no\""
  }
  {
    "short_name": "Clear link to Technology",
    "value_json": "\"no\""
  }
  {
    "short_name": "Harmed Class of Entities",
    "value_json": "true"
  }
  {
    "short_name": "Annotator’s AI special interest intangible harm assessment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Public Sector Deployment",
    "value_json": "\"no\""
  }
  {
    "short_name": "Autonomy Level",
    "value_json": "\"Autonomy1\""
  }
  {
    "short_name": "Intentional Harm",
    "value_json": "\"No. Not intentionally designed to perform harm\""
  }
  {
    "short_name": "AI tools and methods",
    "value_json": "[\"computer vision\"]"
  }
  {
    "short_name": "Peer Reviewer",
    "value_json": "\"002\""
  }
  {
    "short_name": "Quality Control",
    "value_json": "false"
  }
  {
    "short_name": "Annotation Status",
    "value_json": "\"6. Complete and final\""
  }
  {
    "short_name": "Incident Number",
    "value_json": "30"
  }
  {
    "short_name": "Annotator",
    "value_json": "\"\""
  }
  {
    "short_name": "AI Tangible Harm Level Notes",
    "value_json": "\"Delay & financial loss cannot be linked to the performance of the robots but is instead due to a misallocation of resources from management side. \""
  }
  {
    "short_name": "Notes (special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Special Interest Intangible Harm",
    "value_json": "\"no\""
  }
  {
    "short_name": "Notes (AI special interest intangible harm)",
    "value_json": "\"\""
  }
  {
    "short_name": "Date of Incident Year",
    "value_json": "2018"
  }
  {
    "short_name": "Date of Incident Month",
    "value_json": "\"03\""
  }
  {
    "short_name": "Date of Incident Day",
    "value_json": "\"\""
  }
  {
    "short_name": "Estimated Date",
    "value_json": "true"
  }
  {
    "short_name": "Multiple AI Interaction",
    "value_json": "\"no\""
  }
  {
    "short_name": "Embedded",
    "value_json": "\"yes\""
  }
  {
    "short_name": "Location City",
    "value_json": "\"Fremont\""
  }
  {
    "short_name": "Location State/Province (two letters)",
    "value_json": "\"CA\""
  }
  {
    "short_name": "Location Country (two letters)",
    "value_json": "\"US\""
  }
  {
    "short_name": "Location Region",
    "value_json": "\"North America\""
  }
  {
    "short_name": "Infrastructure Sectors",
    "value_json": "[]"
  }
  {
    "short_name": "Operating Conditions",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Environmental and Temporal Characteristics)",
    "value_json": "\"Month corresponds to End of Q1 of 2018, which is when Tesla first reported production shortfalls. \""
  }
  {
    "short_name": "Entities",
    "value_json": "[{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Tesla\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"deployer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"Other harm not meeting CSET definitions\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"financial loss\\\"\"},{\"short_name\":\"Notes (Characterizing Entities and the Harm)\",\"value_json\":\"\\\"financial loss due to misallocation of resources\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Kuka\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"true\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"for-profit organization\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"developer\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]},{\"attributes\":[{\"short_name\":\"Entity\",\"value_json\":\"\\\"Kuka assembly robots\\\"\"},{\"short_name\":\"Named Entity\",\"value_json\":\"false\"},{\"short_name\":\"Entity type\",\"value_json\":\"\\\"product\\\"\"},{\"short_name\":\"Entity Relationship to the AI\",\"value_json\":\"[\\\"product containing AI\\\"]\"},{\"short_name\":\"Harm Category Experienced\",\"value_json\":\"\\\"not applicable\\\"\"},{\"short_name\":\"Harm Type Experienced\",\"value_json\":\"\\\"not applicable\\\"\"}]}]"
  }
  {
    "short_name": "Lives Lost",
    "value_json": "0"
  }
  {
    "short_name": "Injuries",
    "value_json": "0"
  }
  {
    "short_name": "Estimated Harm Quantities",
    "value_json": "false"
  }
  {
    "short_name": "Notes ( Tangible Harm Quantities Information)",
    "value_json": "\"\""
  }
  {
    "short_name": "AI System Description",
    "value_json": "\"Manufacturing robot used in Tesla factories to produce the Model 3 car, performing tasks such as stamping, painting, welding, final assembly, and battery insulation.\""
  }
  {
    "short_name": "Data Inputs",
    "value_json": "\"\""
  }
  {
    "short_name": "Notes (Information about AI System)",
    "value_json": "\"The robots operated independently, but often required human maintenance.\""
  }
  {
    "short_name": "Physical System Type",
    "value_json": "\"Manufacturing Robot\""
  }
  {
    "short_name": "AI Task",
    "value_json": "[\"production\",\"assembly\",\"object detection\"]"
  }
  {
    "short_name": "Notes (AI Functionality and Techniques)",
    "value_json": "\"\""
  }

---

Taxonomy: CSETv1
Classification Count: 6

Based on the incident text and the taxonomy definition provided, provide a classification ONLY for the attribute "Tangible Harm".

IMPORTANT: Your classification MUST include ONLY the following taxonomy attribute:
Tangible Harm

For maximum accuracy and completeness:
1. Focus ONLY on the required field "Tangible Harm".
2. Use the permitted_values for this attribute from the definition provided.
3. Review similar incidents to understand how this specific field is typically used.

Return your response as a JSON object with the following structure:

{
  "classification": {
    "namespace": "CSETv1",
    "attributes": [
      {"short_name": "Tangible Harm", "value_json": ""value""} 
    ]
  },
  "explanation": "A detailed explanation of your classification choice for Tangible Harm.",
  "confidence": "A confidence score between 0 and 1 for this attribute classification"
}

DO NOT include any other text in your response, nor any other characters.
DO NOT start your response with ```json or ```
Ensure that ONLY the attribute "Tangible Harm" is included in your classification.
